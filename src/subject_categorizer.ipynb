{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from helpers  import *\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "#nltk.download('punkt')\n",
    "\n",
    "np.random.seed(2018) # set random seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         \"We have a sitting Democrat US Senator on tria...\n",
       "1         Marshawn Lynch arrives to game in anti-Trump s...\n",
       "2         Daughter of fallen Navy Sailor delivers powerf...\n",
       "3         JUST IN: President Trump dedicates Presidents ...\n",
       "4         19,000 RESPECTING our National Anthem! #StandF...\n",
       "5         Dan Bongino: \"Nobody trolls liberals better th...\n",
       "6                               🐝🐝🐝 https://t.co/MorL3AQW0z\n",
       "7         '@SenatorMenendez @CarmenYulinCruz Doesn't mat...\n",
       "8         As much as I hate promoting CNN article, here ...\n",
       "9         After the 'genocide' remark from San Juan Mayo...\n",
       "10        After the 'genocide' remark from San Juan Mayo...\n",
       "11        '@thehill Why won't she apologize to us for ly...\n",
       "12        Sarah Sanders destroys NBC reporter: \"Trump ma...\n",
       "13        Hi @MichelleObama, remember when you said Wein...\n",
       "14        Hi @MichelleObama, remember when you praised H...\n",
       "15        Wow! Even CNN is slamming the Obamas for silen...\n",
       "16        First lady Melania Trump visits infant opioid ...\n",
       "17        BREAKING: The audio of sexual predator Harvey ...\n",
       "18                       '@Breaking911 Build that wall!! 👍'\n",
       "19        \"It took Hillary abt 5 minutes to blame NRA fo...\n",
       "20        Because he hates women so much, President Trum...\n",
       "21        Congrats to Kirstjen Nielsen, our next Secreta...\n",
       "22        Pres. Trump: \"We want lower taxes, bigger payc...\n",
       "23        I am an Eagle Scout   Only boys should be in B...\n",
       "24        This Eminem freestyle proves that celebs still...\n",
       "25        Do you agree with the Boy Scouts for allowing ...\n",
       "26        Pres. Trump just dropped a NUKE-truth-bomb: \"T...\n",
       "27        Chief of Staff John Kelly destroys liberal rep...\n",
       "28        Network news has become so partisan, distorted...\n",
       "29        So we’ve gone from “Russia literally hacked th...\n",
       "                                ...                        \n",
       "380986    Hit my DM for info on promo.. serious inquirie...\n",
       "380987    Happy 356 day to all my phonks ezzz n pzzz  ew...\n",
       "380988    ��� u will all get ur M4L barbz. Promise. U be...\n",
       "380989    BRAND NEW&gt;&gt;&gt;  \"Keep Lovin You\"  by (A...\n",
       "380990    NEW Tracc ALERT!! #Play #DL #Share #KIC Baller...\n",
       "380991    Listen to Ballerina (remix) by Wyld 7 #np on #...\n",
       "380992    Speaking of @RapperToneLoc, from A Cool Like T...\n",
       "380993    Snuggled up watching Queen @OctaviaSpencer on ...\n",
       "380994    '@AceMackMW @Drake @IamAkademiks @PUSHA_T @Kid...\n",
       "380995             '@AceMackMW @kanyewest @Drake thanknyou'\n",
       "380996    '@AceMackMW @liftedtw @Drake @Spotify @welcome...\n",
       "380997    Jessica's poetry collection is magical. But th...\n",
       "380998    Tune in to @KTVU@TheRedBoothShow for my new sh...\n",
       "380999    CHECK OUT THE HIT SINGLE \"MY BLACK IS BEAUTIFU...\n",
       "381000    Jazz percussionist @david_lyttle talks working...\n",
       "381001    Black Americans are twice as likely as whites ...\n",
       "381002    This happened. Here. In the Triangle. Please r...\n",
       "381003                         Lol! https://t.co/DjlU4Os0yI\n",
       "381004    Powerful stuff here from @sarahkendzior on how...\n",
       "381005    FBI Director Comey asked the DoJ to reject Tru...\n",
       "381006    This song...this show...this season...#ShadesO...\n",
       "381007    The folks who believe/support #Trump never lik...\n",
       "381008      #trumprussia #Impeach45 https://t.co/nrA0fTsSso\n",
       "381009    Trump wants us to believe Obama tapped his pho...\n",
       "381010    I'm so busy I don't know whether I found a rop...\n",
       "381011    ICYMI, @SabrinaAnnLynn dropped a lyric video f...\n",
       "381012    WorldStarHipHop Founder Lee \"Q\" O'Denat Dies a...\n",
       "381013    “This world is but a canvas to our imagination...\n",
       "381014    House Republicans release bill that would dism...\n",
       "381015    Retweeted Citizen TV Kenya (@citizentvkenya): ...\n",
       "Name: content, Length: 291571, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "tweets1 = pd.read_csv(access_folder('data') + 'IRAhandle_tweets_1.csv')\n",
    "\n",
    "# selecting content columns for subject categorization (all should be in english)\n",
    "content = tweets1[tweets1.language == 'English'].content\n",
    "\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization: Split text into words. Lowercase the words and remove punctuation\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\") # initiate dictionary type\n",
    "# create words from sentences\n",
    "content_tokens = [nltk.word_tokenize(x) for x in content]\n",
    "# Stem words (truncate)\n",
    "content_stemmed = []\n",
    "for tweet in content_tokens:\n",
    "    stem = [stemmer.stem(x) for x in tweet if len(x) > 3] # only consider words more than 3 letters\n",
    "    content_stemmed.append(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary to know the frequency of words\n",
    "dictionary = gensim.corpora.Dictionary(content_stemmed)\n",
    "dictionary.filter_extremes(no_below=15) # remove words that have appearances less than 15 times\n",
    "\n",
    "# Create bag-of-words\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in content_stemmed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shit is working\n",
    "test = bow_corpus[126]\n",
    "for i in range(len(test)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(test[i][0],dictionary[test[i][0]], \n",
    "test[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model on bag-of-words\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
