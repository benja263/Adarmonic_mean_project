{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "from helpers import *\n",
    "from LDA_helpers import *\n",
    "\n",
    "np.random.seed(2018) # set random seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "tweets1 = pd.read_csv(access_folder('data') + 'IRAhandle_tweets_1.csv')\n",
    "tweets2 = pd.read_csv(access_folder('data') + 'IRAhandle_tweets_2.csv')\n",
    "tweets3 = pd.read_csv(access_folder('data') + 'IRAhandle_tweets_3.csv')\n",
    "# tweets4 = pd.read_csv(access_folder('data') + 'IRAhandle_tweets_4.csv') # process function throws error\n",
    "tweets5 = pd.read_csv(access_folder('data') + 'IRAhandle_tweets_5.csv')\n",
    "tweets6 = pd.read_csv(access_folder('data') + 'IRAhandle_tweets_6.csv')\n",
    "\n",
    "\n",
    "tweets = pd.concat([tweets1, tweets2, tweets3, tweets5, tweets6], axis=0, join='outer', join_axes=None, ignore_index=False,\n",
    "          keys=None, levels=None, names=None, verify_integrity=False,\n",
    "          copy=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1803466, 15)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hashtagged, tokens, hashtag_column = process_tweets(tweets, group_by='hashtag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90823"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                          \n",
       "1                                                          \n",
       "2                                               #BoycottNFL\n",
       "3                                                          \n",
       "4                                        #StandForOurAnthem\n",
       "5                                                          \n",
       "6                                                          \n",
       "7                                                          \n",
       "8                                                          \n",
       "9                                                          \n",
       "10                                                         \n",
       "11                                                         \n",
       "12                                                         \n",
       "13                                                         \n",
       "14                                                         \n",
       "15                                                         \n",
       "16                                                         \n",
       "17                                                         \n",
       "18                                                         \n",
       "19                                                         \n",
       "20                                                         \n",
       "21                                                         \n",
       "22                                                         \n",
       "23                                                         \n",
       "24                                            #HipHopAwards\n",
       "25                                                         \n",
       "26                                                         \n",
       "27                                                         \n",
       "28                                                         \n",
       "29                                                         \n",
       "                                ...                        \n",
       "305792    #kanyewest #obama #funny #comedy #edit #lol #r...\n",
       "305793                                                     \n",
       "305794    #trump #president #lmao #9gag #president #redd...\n",
       "305795    #obama #standup #lol #9gag #reddit #comedy #re...\n",
       "305982                                                     \n",
       "313163                                                     \n",
       "313221                                                     \n",
       "313423                                                     \n",
       "313427                                                     \n",
       "314849                                                     \n",
       "315732                                                     \n",
       "315812                                                     \n",
       "315872                                                     \n",
       "317124                                                     \n",
       "317421                                                     \n",
       "317825                                                     \n",
       "317847                                                     \n",
       "318557                                                     \n",
       "318861                                                     \n",
       "318930                                                     \n",
       "318960                                                     \n",
       "319984                                                     \n",
       "320911                                                     \n",
       "321469                                                     \n",
       "321704                                                     \n",
       "322807                                                     \n",
       "323242                                                     \n",
       "323304                                                     \n",
       "323717                                                     \n",
       "323775                                                     \n",
       "Name: content, Length: 1268270, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtag_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                                                                    738429\n",
       "#news                                                                                                23191\n",
       "#politics                                                                                            22569\n",
       "#sports                                                                                              16661\n",
       "#health                                                                                               8034\n",
       "#business                                                                                             6764\n",
       "#SanJose                                                                                              3971\n",
       "#top                                                                                                  3780\n",
       "#crime                                                                                                3502\n",
       "#breaking                                                                                             3354\n",
       "#local #news                                                                                          3043\n",
       "#MAGA                                                                                                 2945\n",
       "#BlackLivesMatter                                                                                     2655\n",
       "#Foke                                                                                                 2409\n",
       "#ToDoListBeforeChristmas                                                                              2292\n",
       "#local #SanDiego                                                                                      2287\n",
       "#Breaking                                                                                             2175\n",
       "#Wisconsin                                                                                            2073\n",
       "#ThingsYouCantIgnore                                                                                  1883\n",
       "#mar                                                                                                  1866\n",
       "#topl                                                                                                 1795\n",
       "#IHatePokemonGoBecause                                                                                1719\n",
       "#2016In4Words                                                                                         1637\n",
       "#IGetDepressedWhen                                                                                    1627\n",
       "#MustBeBanned                                                                                         1578\n",
       "#NowPlaying                                                                                           1512\n",
       "#AlternativeAcronymInterpretations                                                                    1468\n",
       "#USFA                                                                                                 1436\n",
       "#local                                                                                                1361\n",
       "#vvmar #marv                                                                                          1341\n",
       "                                                                                                     ...  \n",
       "#mar #RecallMcCain #FireFlake                                                                            1\n",
       "#FollowTheWhiteRabbit #QAnon #TheStormIsComing #WhenDoBirdsSing                                          1\n",
       "#WeBroke                                                                                                 1\n",
       "#McMaster #Trump #NorthKorea #UnitedStates                                                               1\n",
       "#PrayForDev #OscarNoms                                                                                   1\n",
       "#StopIslam #IslamKills #Refugees                                                                         1\n",
       "#Flippa #Domainname #Domainnames #domains #NewGtld                                                       1\n",
       "#envymg                                                                                                  1\n",
       "#vip #MAGA #top                                                                                          1\n",
       "#NOwar #peace                                                                                            1\n",
       "#DailyMotion #Hacked #Accounts #Stolen #Vuln #Hacking #Hacker #Security #Technology                      1\n",
       "#stl #StayWoke #BlackLivesMatter                                                                         1\n",
       "#Business #OrangeCountys #Networking #OCFair                                                             1\n",
       "#israelyou                                                                                               1\n",
       "#SemperFi #USMCR100                                                                                      1\n",
       "#KOT                                                                                                     1\n",
       "#IndianaShooting                                                                                         1\n",
       "#ACHA #Dems #GOP                                                                                         1\n",
       "#NotMyPresident #TrumpProtest #GetALife #Trump                                                           1\n",
       "#Jesus #freedom #Christian #ccot                                                                         1\n",
       "#lockherup #TrumpForPresident                                                                            1\n",
       "#sp #strange                                                                                             1\n",
       "#TrumpTrain #TrumpsArmy                                                                                  1\n",
       "#vets4childrescueorg                                                                                     1\n",
       "#BlackAmerican #Canada #PoliceBrutality #Blackmatters                                                    1\n",
       "#MAGA #CNNLEAKS                                                                                          1\n",
       "#Democrats2020                                                                                           1\n",
       "#ThursdayThought #EpiPen                                                                                 1\n",
       "#StopIslam #Brussels #Belgium #ThisIsIslam #IslamKills #Muslims #Islamophobia #Islam #IslamKills         1\n",
       "#DannySanchez                                                                                            1\n",
       "Name: content, Length: 90823, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(hashtag_column)\n",
    "hashtag_column.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_sport = data_hashtagged.index.get_loc(\"#sports\")\n",
    "ind_MAGA = data_hashtagged.index.get_loc('#MAGA')\n",
    "ind_BlackLivesMatter = data_hashtagged.index.get_loc('#BlackLivesMatter')\n",
    "ind_business = data_hashtagged.index.get_loc('#business')\n",
    "ind_USFA = data_hashtagged.index.get_loc('#USFA')\n",
    "\n",
    "# Concatenate the sublists into one to feed to the model\n",
    "list_concat = []\n",
    "list_concat.append(tokens[ind_sport])\n",
    "list_concat.append(tokens[ind_MAGA])\n",
    "list_concat.append(tokens[ind_BlackLivesMatter])\n",
    "list_concat.append(tokens[ind_business])\n",
    "list_concat.append(tokens[ind_USFA])\n",
    "data_processed = list_concat.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of sum of frequencies across all author IDs\n",
    "dict1 = dict(hashtag_column.sum())\n",
    "# Export into pandas for ordering\n",
    "hashtag_frequencies = pd.DataFrame.from_dict(dict1, orient='index', columns=['count'])\n",
    "hashtag_frequencies.sort_values(['count'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sports, #MAGA, #BlackLivesMatter, #business, #USFA\n",
    "# Load data\n",
    "datafolder = './data/'\n",
    "tweets1 = pd.read_csv(datafolder + 'IRAhandle_tweets_1.csv')\n",
    "tweets2 = pd.read_csv(datafolder + 'IRAhandle_tweets_2.csv')\n",
    "tweets = pd.concat([tweets1, tweets2], axis=0, join='outer', join_axes=None, ignore_index=False,\n",
    "          keys=None, levels=None, names=None, verify_integrity=False,\n",
    "          copy=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary to know the frequency of words\n",
    "id2word = gensim.corpora.Dictionary(data_processed)\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in data_processed]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "# lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "#                                            id2word=id2word,\n",
    "#                                            num_topics=20, \n",
    "#                                            random_state=100,\n",
    "#                                            update_every=1,\n",
    "#                                            chunksize=100,\n",
    "#                                            passes=10,\n",
    "#                                            alpha='auto',\n",
    "#                                            per_word_topics=True)\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(corpus = corpus,\n",
    "                                       num_topics=6,\n",
    "                                       id2word=id2word,\n",
    "                                       chunksize=2000,\n",
    "                                       passes=10, \n",
    "                                       per_word_topics=False,\n",
    "                                       workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda_model.print_topics())\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_processed, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_path = '/Users/halimaschede/Documents/GitHub/Adarmonic_mean_project/data/mallet-2.0.8/bin/mallet' # update this path\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=15, id2word=id2word)\n",
    "\n",
    "# Show Topics\n",
    "print(ldamallet.show_topics(formatted=False))\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=content_stemmed, dictionary=id2word, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_ldamallet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shit is working\n",
    "test = bow_corpus[126]\n",
    "for i in range(len(test)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(test[i][0],dictionary[test[i][0]], \n",
    "test[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model on bag-of-words\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=20, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda_model.show_topic(0))\n",
    "# for topic in range(10):\n",
    "#     plt.figure()\n",
    "#     plt.imshow(WordCloud().fit_words(lda_model.show_topic(topic)))\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.title(\"Topic #\" + str(topic))\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada]",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
