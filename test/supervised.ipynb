{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model for text classification using a neural network to assign class probabilities\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "# from tensorflow.contrib import learn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "# from gensim.parsing.preprocessing import remove_stopwords\n",
    "# import Pattern\n",
    "\n",
    "\n",
    "##### RUN FOLLOWING TO GENERATE THE CSV FILE WITH TWEET CONTENT AND HASHTAGS\n",
    "##### OTHERWISE LOAD THE SAVED DATAFRAME\n",
    "\n",
    "# data = pd.read_csv('../data/ira_tweets_new/rus_troll_tweet_text.csv')\n",
    "#english_tweets = data[data.tweet_language == 'en']\n",
    "#hashtag_info = pd.read_csv('../data/ira_tweets_new/rus_troll_tweet_stats.csv')[['tweetid', 'hashtags']]\n",
    "#merged = pd.merge(english_tweets, hashtag_info, on='tweetid')[['tweet_text', 'hashtags']]\n",
    "\n",
    "# # Remove tweets that have no hashtag\n",
    "# data.replace('[]',np.nan, inplace=True)\n",
    "# data.dropna(how='any', inplace=True)\n",
    "\n",
    "# # Remove hashtags and other tags and grammar stuff\n",
    "# data_clean=data.replace('#\\w+', '', regex=True).replace('@\\w+', '', regex=True).replace('RT ', '', regex=True)\n",
    "# data_clean.tweet_text = data_clean.tweet_text.apply(lambda x: re.sub(r'http\\S+', '', x)).apply(lambda x: re.sub(r\"'|\\\"|`|:|\\?|~|,|\\.\", '', x))\n",
    "# data_clean.hashtags = data_clean.hashtags.apply(lambda x: x[1:-1]) # make list in hashtag column\n",
    "# duplicated = pd.DataFrame(data_clean.hashtags.str.split(', ').tolist(), index=data_clean.tweet_text).stack()\n",
    "# duplicated = duplicated.reset_index()[[0, 'tweet_text']] # var1 variable is currently labeled 0\n",
    "# duplicated.columns = ['hashtags', 'tweet_text'] # renaming var1\n",
    "\n",
    "# # Show top hashtags\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#     print(duplicated.hashtags.value_counts()[0:300])\n",
    "    \n",
    "# # Remove stop words\n",
    "# duplicated.tweet_text = duplicated.tweet_text.apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "\n",
    "# # Filter dataframe for hashtags identified to be in a topic\n",
    "\n",
    "# # Create list of relevant hashtags\n",
    "# hashtag_list = ['MAGA', 'WakeUpAmerica', 'MakeAmericaGreatAgain', \n",
    "#                 'ARRESTObama', 'TrumpTrain', 'pjnet', 'PJNET', 'Trump2016',\n",
    "#                 'maga', 'TrumpForPresident', 'TrumpBecause', 'ObamaLameDuck',\n",
    "#                 'ICelebrateTrumpWith', 'BlackLivesMatter', 'BlackTwitter', 'blacklivesmatter', \n",
    "#                 'BlackHistoryMonth', 'BlackSkinIsNotACrime',  'blacktolive', 'BlackToLive', 'blacktwitter',\n",
    "#                 'tech', 'science', 'entertainment', 'celebs', 'MyEmmyNominationWouldBe', 'soundcloud', 'iTunes',\n",
    "#                 'rap', 'health', 'sports', 'hockey', 'Sports', 'MyOlympicSportWouldBe', 'ISIS', 'IslamKills',\n",
    "#                 'StopIslam', 'Hillary', 'ThingsMoreTrustedThanHillary', 'NeverHillary', 'CrookedHillary',\n",
    "#                 'world', 'Syria', 'Turkey', 'Brussels', 'Russia', 'Iraq', 'business', 'news', 'local', \n",
    "#                 'Fukushima2015', 'FukushimaAgain', 'FUKUSHIMA2015', 'fukushima2015', 'IHatePokemonGoBecause',\n",
    "#                 'crime', 'PoliceBrutality', 'USFA', 'ArmyWomenProblems']\n",
    "\n",
    "# # Filter dataframe\n",
    "# filtered = duplicated[duplicated.hashtags.isin(hashtag_list)]\n",
    "\n",
    "\n",
    "# # Lemmatize the words and also keeping their context (verb, noun etc)\n",
    "\n",
    "# from gensim.utils import lemmatize\n",
    "# filtered.tweet_text = filtered.tweet_text.apply(lambda x: lemmatize(x, min_length=3))\n",
    "\n",
    "\n",
    "# # change hashtag names to the topic names\n",
    "# pd.options.mode.chained_assignment = None  # default='warn' \n",
    "\n",
    "# # General News\n",
    "# filtered.hashtags.loc[filtered[(filtered.hashtags == 'news') |\n",
    "#                                (filtered.hashtags == 'local')].index.values] = 'General News'\n",
    "\n",
    "# # Fukushima\n",
    "# filtered.hashtags.loc[filtered[(filtered.hashtags == 'Fukushima2015') |\n",
    "#                                (filtered.hashtags == 'FukushimaAgain') |\n",
    "#                               (filtered.hashtags == 'FUKUSHIMA2015') | \n",
    "#                               (filtered.hashtags == 'fukushima2015')].index.values] = 'Fukushima'\n",
    "\n",
    "# # Crime\n",
    "# filtered.hashtags.loc[filtered[(filtered.hashtags == 'crime') |\n",
    "#                                (filtered.hashtags == 'PoliceBrutality')].index.values] = 'Crime'\n",
    "\n",
    "# # Gun Related\n",
    "# filtered.hashtags.loc[filtered[(filtered.hashtags == 'USFA') |\n",
    "#                                (filtered.hashtags == 'ArmyWomenProblems')].index.values] = 'Gun Related'\n",
    "\n",
    "# # Anti-Islam\n",
    "# filtered.hashtags.loc[filtered[(filtered.hashtags == 'ISIS') |\n",
    "#                                (filtered.hashtags == 'IslamKills') |\n",
    "#                               (filtered.hashtags == 'StopIslam')].index.values] = 'Anti-Islam'\n",
    "\n",
    "# # Anti-Hillary\n",
    "# filtered.hashtags.loc[filtered[(filtered.hashtags == 'Hillary') |\n",
    "#                                (filtered.hashtags == 'ThingsMoreTrustedThanHillary') |\n",
    "#                               (filtered.hashtags == 'NeverHillary') |\n",
    "#                               (filtered.hashtags == 'CrookedHillary')].index.values] = 'Anti-Hillary'\n",
    "\n",
    "\n",
    "# # Foreign Countries\n",
    "# filtered.hashtags.loc[filtered[(filtered.hashtags == 'world') |\n",
    "#                                (filtered.hashtags == 'Syria') |\n",
    "#                               (filtered.hashtags == 'Turkey') |\n",
    "#                               (filtered.hashtags == 'Brussels') |\n",
    "#                               (filtered.hashtags == 'Russia') |\n",
    "#                               (filtered.hashtags == 'Iraq')].index.values] = 'Foreign Countries'\n",
    "\n",
    "# # Entertainment\n",
    "# filtered.hashtags.loc[filtered[(filtered.hashtags == 'entertainment') |\n",
    "#                                (filtered.hashtags == 'celebs') |\n",
    "#                               (filtered.hashtags == 'MyEmmyNominationWouldBe') |\n",
    "#                               (filtered.hashtags == 'soundcloud') |\n",
    "#                               (filtered.hashtags == 'iTunes') |\n",
    "#                               (filtered.hashtags == 'rap')].index.values] = 'Entertainment'\n",
    "\n",
    "\n",
    "# # Black Race Support\n",
    "# filtered.hashtags.loc[filtered[(filtered.hashtags == 'BlackLivesMatter') |\n",
    "#                                (filtered.hashtags == 'BlackTwitter') |\n",
    "#                               (filtered.hashtags == 'blacklivesmatter') |\n",
    "#                               (filtered.hashtags == 'BlackHistoryMonth') |\n",
    "#                               (filtered.hashtags == 'BlackSkinIsNotACrime') |\n",
    "#                               (filtered.hashtags == 'blacktolive') |\n",
    "#                               (filtered.hashtags == 'BlackToLive') |\n",
    "#                               (filtered.hashtags == 'blacktwitter')].index.values] = 'Black Support'\n",
    "\n",
    "# # Trump Support\n",
    "# filtered.hashtags.loc[filtered[(filtered.hashtags == 'MAGA') |\n",
    "#                                (filtered.hashtags == 'WakeUpAmerica') |\n",
    "#                               (filtered.hashtags == 'MakeAmericaGreatAgain') |\n",
    "#                               (filtered.hashtags == 'ARRESTObama') |\n",
    "#                               (filtered.hashtags == 'TrumpTrain') |\n",
    "#                               (filtered.hashtags == 'pjnet') |\n",
    "#                               (filtered.hashtags == 'PJNET') |\n",
    "#                               (filtered.hashtags == 'Trump2016') |\n",
    "#                               (filtered.hashtags == 'maga') |\n",
    "#                               (filtered.hashtags == 'TrumpForPresident') |\n",
    "#                               (filtered.hashtags == 'TrumpBecause') |\n",
    "#                               (filtered.hashtags == 'ObamaLameDuck') |\n",
    "#                               (filtered.hashtags == 'ICelebrateTrumpWith')].index.values] = 'Trump Support'\n",
    "\n",
    "\n",
    "# # Technology\n",
    "# filtered.hashtags.loc[filtered[(filtered.hashtags == 'tech') |\n",
    "#                                (filtered.hashtags == 'science')].index.values] = 'Technology'\n",
    "\n",
    "# # Health\n",
    "# filtered.hashtags.loc[filtered[filtered.hashtags == 'health'].index.values] = 'Health'\n",
    "\n",
    "# # Sports\n",
    "# filtered.hashtags.loc[filtered[(filtered.hashtags == 'sports') |\n",
    "#                                (filtered.hashtags == 'hockey') |\n",
    "#                               (filtered.hashtags == 'Sports') |\n",
    "#                               (filtered.hashtags == 'MyOlympicSportWouldBe')].index.values] = 'Sports'\n",
    "\n",
    "\n",
    "# # Business\n",
    "# filtered.hashtags.loc[filtered[filtered.hashtags == 'business'].index.values] = 'Business'\n",
    "\n",
    "# # Video GAmes\n",
    "# filtered.hashtags.loc[filtered[filtered.hashtags == 'IHatePokemonGoBecause'].index.values] = 'Video Games'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load english dataframe\n",
    "filtered = pd.read_csv('../data/tweets_n_hashtags.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>General News</td>\n",
       "      <td>[b'john/NN', b'carroll/NN', b'university/NN', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>General News</td>\n",
       "      <td>[b'free/JJ', b'spring/NN', b'cook/VB', b'book/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>General News</td>\n",
       "      <td>[b'rocky/JJ', b'river/NN', b'prepare/VB', b'fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>General News</td>\n",
       "      <td>[b'forecast/VB', b'cooler/JJ', b'mid/JJ', b'we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>General News</td>\n",
       "      <td>[b'arrest/VB', b'connection/NN', b'art/NN', b'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        hashtags                                         tweet_text\n",
       "33  General News  [b'john/NN', b'carroll/NN', b'university/NN', ...\n",
       "34  General News  [b'free/JJ', b'spring/NN', b'cook/VB', b'book/...\n",
       "37  General News  [b'rocky/JJ', b'river/NN', b'prepare/VB', b'fl...\n",
       "39  General News  [b'forecast/VB', b'cooler/JJ', b'mid/JJ', b'we...\n",
       "42  General News  [b'arrest/VB', b'connection/NN', b'art/NN', b'..."
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered.tweet_text = filtered.tweet_text.apply(lambda x: x[1:-1]).replace(\"b'\", '', regex=True).replace(\"'\", '', regex=True)\n",
    "filtered.tweet_text = filtered.tweet_text.apply(lambda x: x.split(', '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>General News</td>\n",
       "      <td>[john/NN, carroll/NN, university/NN, get/VB, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>General News</td>\n",
       "      <td>[free/JJ, spring/NN, cook/VB, book/NN, healthy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>General News</td>\n",
       "      <td>[rocky/JJ, river/NN, prepare/VB, flooding/JJ, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>General News</td>\n",
       "      <td>[forecast/VB, cooler/JJ, mid/JJ, week/NN, cool...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>General News</td>\n",
       "      <td>[arrest/VB, connection/NN, art/NN, festival/NN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>General News</td>\n",
       "      <td>[horror/NN, director/NN, craven/JJ, die/VB, sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Sports</td>\n",
       "      <td>[secretariat/VB, dominate/VB, american/JJ, pha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>General News</td>\n",
       "      <td>[black/JJ, box/NN, miss/VB, german/NN, airline...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Sports</td>\n",
       "      <td>[dale/NN, earnhardt/NN, get/VB, engage/VB, gir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Sports</td>\n",
       "      <td>[brown/NN, mingo/NN, undergo/VB, surgery/NN, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>General News</td>\n",
       "      <td>[avenger/NN, explode/VB, thursday/NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Crime</td>\n",
       "      <td>[clean/JJ, sweep/NN, vacuum/NN, cleaner/JJ, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>General News</td>\n",
       "      <td>[state/NN, senate/NN, expect/VB, vote/NN, comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Technology</td>\n",
       "      <td>[ancient/JJ, loch/NN, ness/NN, monster/NN, rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Sports</td>\n",
       "      <td>[johns/RB, take/VB, goodell/NN, trust/VB, new/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Crime</td>\n",
       "      <td>[cleveland/JJ, height/NN, police/NN, look/VB, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Crime</td>\n",
       "      <td>[medina/JJ, man/NN, didnt/VB, use/NN, trash/NN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Entertainment</td>\n",
       "      <td>[best/RB, cast/VB, director/NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Entertainment</td>\n",
       "      <td>[take/VB, hand/NN, add/VB, plan/NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Anti-Hillary</td>\n",
       "      <td>[not/RB, voting/NN, voting/NN, anyone/NN, othe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Fukushima</td>\n",
       "      <td>[god/NN, help/VB, sound/VB, absurd/JJ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Fukushima</td>\n",
       "      <td>[thermal/JJ, nuclear/JJ, power/NN, plant/NN, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>General News</td>\n",
       "      <td>[richard/NN, kivett/VB, elected/JJ, mayor/NN, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Foreign Countries</td>\n",
       "      <td>[yeman/NN, fighter/NN, aden/VB, tawahi/JJ, dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>General News</td>\n",
       "      <td>[yeman/NN, fighter/NN, aden/VB, tawahi/JJ, dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>General News</td>\n",
       "      <td>[agriculture/NN, equestrian/JJ, center/NN, tak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>General News</td>\n",
       "      <td>[agriculture/NN, equestrian/JJ, center/NN, tak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>General News</td>\n",
       "      <td>[arrest/VB, polouse/NN, drive/VB, stolen/JJ, v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>General News</td>\n",
       "      <td>[saint/NN, patriot/NN, joint/JJ, practice/NN, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Entertainment</td>\n",
       "      <td>[gon/NN, mom/NN, hooo/NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150327</th>\n",
       "      <td>Trump Support</td>\n",
       "      <td>[that/VB, elected/JJ, trump/NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150334</th>\n",
       "      <td>Anti-Hillary</td>\n",
       "      <td>[video/NN, thx/NN, leah/RB, destroy/VB, amp/NN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150335</th>\n",
       "      <td>Black Support</td>\n",
       "      <td>[cocaine/NN, colored/JJ, killer/NN, create/VB,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150377</th>\n",
       "      <td>Sports</td>\n",
       "      <td>[fangirling/JJ, try/VB, scream/NN, time/NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150389</th>\n",
       "      <td>Sports</td>\n",
       "      <td>[not/RB, give/VB, fuck/VB, olympic/NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150393</th>\n",
       "      <td>Sports</td>\n",
       "      <td>[couch/NN, potato/NN, game/NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150408</th>\n",
       "      <td>Sports</td>\n",
       "      <td>[fangirling/JJ, try/VB, scream/NN, time/NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150423</th>\n",
       "      <td>Video Games</td>\n",
       "      <td>[what/VB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150457</th>\n",
       "      <td>Sports</td>\n",
       "      <td>[not/RB, give/VB, fuck/VB, olympic/NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150486</th>\n",
       "      <td>Black Support</td>\n",
       "      <td>[common/JJ, color/NN, blue/NN, youre/NN, dont/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150488</th>\n",
       "      <td>Trump Support</td>\n",
       "      <td>[worst/JJ, america/NN, accord/VB, guess/NN, ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150489</th>\n",
       "      <td>Anti-Hillary</td>\n",
       "      <td>[worst/JJ, america/NN, accord/VB, guess/NN, ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150518</th>\n",
       "      <td>Sports</td>\n",
       "      <td>[fangirling/JJ, try/VB, scream/NN, time/NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150520</th>\n",
       "      <td>Trump Support</td>\n",
       "      <td>[president/NN, put/VB, safety/NN, american/NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150533</th>\n",
       "      <td>Trump Support</td>\n",
       "      <td>[prophet/NN, lie/VB, kill/JJ, amp/NN, rob/VB, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150558</th>\n",
       "      <td>Gun Related</td>\n",
       "      <td>[want/VB, hold/JJ, hand/NN, battle/VB, cuddle/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150564</th>\n",
       "      <td>Gun Related</td>\n",
       "      <td>[divorce/NN, man/NN, friendly/JJ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150570</th>\n",
       "      <td>Sports</td>\n",
       "      <td>[fangirling/JJ, try/VB, scream/NN, time/NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150603</th>\n",
       "      <td>Black Support</td>\n",
       "      <td>[victory/NN, fight/NN, isnt/VB, good/JJ, momen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150606</th>\n",
       "      <td>Trump Support</td>\n",
       "      <td>[remember/NN, say/VB, wasnt/VB, salary/NN, acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150626</th>\n",
       "      <td>Gun Related</td>\n",
       "      <td>[not/RB, allow/VB, carry/JJ, gun/NN, vagina/NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150638</th>\n",
       "      <td>Gun Related</td>\n",
       "      <td>[male/NN, actually/RB, work/VB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150651</th>\n",
       "      <td>Gun Related</td>\n",
       "      <td>[get/VB, bullet/NN, bullet/NN, man/NN, get/VB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150667</th>\n",
       "      <td>Anti-Hillary</td>\n",
       "      <td>[come/VB, already/RB, really/RB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150771</th>\n",
       "      <td>Sports</td>\n",
       "      <td>[pin/JJ, tail/NN, toupee/NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150786</th>\n",
       "      <td>Trump Support</td>\n",
       "      <td>[president/NN, trump/NN, amazing/JJ, address/N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150795</th>\n",
       "      <td>Sports</td>\n",
       "      <td>[fangirling/JJ, try/VB, scream/NN, time/NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150827</th>\n",
       "      <td>Sports</td>\n",
       "      <td>[fangirling/JJ, try/VB, scream/NN, time/NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150834</th>\n",
       "      <td>Gun Related</td>\n",
       "      <td>[not/RB, allow/VB, carry/JJ, gun/NN, vagina/NN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150849</th>\n",
       "      <td>Entertainment</td>\n",
       "      <td>[what/VB, emmy/RB]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>662052 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  hashtags                                         tweet_text\n",
       "33            General News  [john/NN, carroll/NN, university/NN, get/VB, g...\n",
       "34            General News  [free/JJ, spring/NN, cook/VB, book/NN, healthy...\n",
       "37            General News  [rocky/JJ, river/NN, prepare/VB, flooding/JJ, ...\n",
       "39            General News  [forecast/VB, cooler/JJ, mid/JJ, week/NN, cool...\n",
       "42            General News  [arrest/VB, connection/NN, art/NN, festival/NN...\n",
       "44            General News  [horror/NN, director/NN, craven/JJ, die/VB, sc...\n",
       "45                  Sports  [secretariat/VB, dominate/VB, american/JJ, pha...\n",
       "46            General News  [black/JJ, box/NN, miss/VB, german/NN, airline...\n",
       "48                  Sports  [dale/NN, earnhardt/NN, get/VB, engage/VB, gir...\n",
       "50                  Sports  [brown/NN, mingo/NN, undergo/VB, surgery/NN, b...\n",
       "51            General News              [avenger/NN, explode/VB, thursday/NN]\n",
       "52                   Crime  [clean/JJ, sweep/NN, vacuum/NN, cleaner/JJ, th...\n",
       "53            General News  [state/NN, senate/NN, expect/VB, vote/NN, comm...\n",
       "54              Technology  [ancient/JJ, loch/NN, ness/NN, monster/NN, rep...\n",
       "56                  Sports  [johns/RB, take/VB, goodell/NN, trust/VB, new/...\n",
       "57                   Crime  [cleveland/JJ, height/NN, police/NN, look/VB, ...\n",
       "58                   Crime  [medina/JJ, man/NN, didnt/VB, use/NN, trash/NN...\n",
       "60           Entertainment                    [best/RB, cast/VB, director/NN]\n",
       "67           Entertainment                [take/VB, hand/NN, add/VB, plan/NN]\n",
       "71            Anti-Hillary  [not/RB, voting/NN, voting/NN, anyone/NN, othe...\n",
       "86               Fukushima             [god/NN, help/VB, sound/VB, absurd/JJ]\n",
       "88               Fukushima  [thermal/JJ, nuclear/JJ, power/NN, plant/NN, u...\n",
       "89            General News  [richard/NN, kivett/VB, elected/JJ, mayor/NN, ...\n",
       "102      Foreign Countries  [yeman/NN, fighter/NN, aden/VB, tawahi/JJ, dis...\n",
       "103           General News  [yeman/NN, fighter/NN, aden/VB, tawahi/JJ, dis...\n",
       "104           General News  [agriculture/NN, equestrian/JJ, center/NN, tak...\n",
       "105           General News  [agriculture/NN, equestrian/JJ, center/NN, tak...\n",
       "106           General News  [arrest/VB, polouse/NN, drive/VB, stolen/JJ, v...\n",
       "107           General News  [saint/NN, patriot/NN, joint/JJ, practice/NN, ...\n",
       "119          Entertainment                          [gon/NN, mom/NN, hooo/NN]\n",
       "...                    ...                                                ...\n",
       "2150327      Trump Support                    [that/VB, elected/JJ, trump/NN]\n",
       "2150334       Anti-Hillary  [video/NN, thx/NN, leah/RB, destroy/VB, amp/NN...\n",
       "2150335      Black Support  [cocaine/NN, colored/JJ, killer/NN, create/VB,...\n",
       "2150377             Sports        [fangirling/JJ, try/VB, scream/NN, time/NN]\n",
       "2150389             Sports             [not/RB, give/VB, fuck/VB, olympic/NN]\n",
       "2150393             Sports                     [couch/NN, potato/NN, game/NN]\n",
       "2150408             Sports        [fangirling/JJ, try/VB, scream/NN, time/NN]\n",
       "2150423        Video Games                                          [what/VB]\n",
       "2150457             Sports             [not/RB, give/VB, fuck/VB, olympic/NN]\n",
       "2150486      Black Support  [common/JJ, color/NN, blue/NN, youre/NN, dont/...\n",
       "2150488      Trump Support  [worst/JJ, america/NN, accord/VB, guess/NN, ha...\n",
       "2150489       Anti-Hillary  [worst/JJ, america/NN, accord/VB, guess/NN, ha...\n",
       "2150518             Sports        [fangirling/JJ, try/VB, scream/NN, time/NN]\n",
       "2150520      Trump Support     [president/NN, put/VB, safety/NN, american/NN]\n",
       "2150533      Trump Support  [prophet/NN, lie/VB, kill/JJ, amp/NN, rob/VB, ...\n",
       "2150558        Gun Related  [want/VB, hold/JJ, hand/NN, battle/VB, cuddle/...\n",
       "2150564        Gun Related                  [divorce/NN, man/NN, friendly/JJ]\n",
       "2150570             Sports        [fangirling/JJ, try/VB, scream/NN, time/NN]\n",
       "2150603      Black Support  [victory/NN, fight/NN, isnt/VB, good/JJ, momen...\n",
       "2150606      Trump Support  [remember/NN, say/VB, wasnt/VB, salary/NN, acc...\n",
       "2150626        Gun Related    [not/RB, allow/VB, carry/JJ, gun/NN, vagina/NN]\n",
       "2150638        Gun Related                    [male/NN, actually/RB, work/VB]\n",
       "2150651        Gun Related     [get/VB, bullet/NN, bullet/NN, man/NN, get/VB]\n",
       "2150667       Anti-Hillary                   [come/VB, already/RB, really/RB]\n",
       "2150771             Sports                       [pin/JJ, tail/NN, toupee/NN]\n",
       "2150786      Trump Support  [president/NN, trump/NN, amazing/JJ, address/N...\n",
       "2150795             Sports        [fangirling/JJ, try/VB, scream/NN, time/NN]\n",
       "2150827             Sports        [fangirling/JJ, try/VB, scream/NN, time/NN]\n",
       "2150834        Gun Related    [not/RB, allow/VB, carry/JJ, gun/NN, vagina/NN]\n",
       "2150849      Entertainment                                 [what/VB, emmy/RB]\n",
       "\n",
       "[662052 rows x 2 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['General News', 'Sports', 'Crime', 'Technology', 'Entertainment', \n",
    "              'Anti-Hillary', 'Fukushima', 'Foreign Countries', 'Trump Support',\n",
    "              'Gun Related', 'Video Games', 'Health', 'Business',\n",
    "              'Black Support', 'Anti-Islam']\n",
    "from collections import Counter\n",
    "\n",
    "vocab = Counter()\n",
    "\n",
    "for text in filtered.tweet_text:\n",
    "    for word in text: ## change this because the texts are already a list of words\n",
    "        vocab[word[2:-1].lower()]+=1\n",
    "\n",
    "total_words = len(vocab)\n",
    "\n",
    "def get_word_2_index(vocab):\n",
    "    word2index = {}\n",
    "    for i,word in enumerate(vocab):\n",
    "        word2index[word.lower()] = i\n",
    "\n",
    "    return word2index\n",
    "\n",
    "word2index = get_word_2_index(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87510"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(df,i,batch_size):\n",
    "    batches = []\n",
    "    results = []\n",
    "    texts = df.tweet_text[i*batch_size:i*batch_size+batch_size]\n",
    "    categories = df.hashtags[i*batch_size:i*batch_size+batch_size]\n",
    "    for text in texts:\n",
    "        layer = np.zeros(total_words,dtype=float) # total words is not defined\n",
    "        for word in text:\n",
    "            layer[word2index[word[2:-1].lower()]] += 1\n",
    "\n",
    "        batches.append(layer)\n",
    "\n",
    "    for category in categories:\n",
    "        index_y = -1\n",
    "        if category == 'General News':\n",
    "            index_y = 0\n",
    "        elif category == 'Sports':\n",
    "            index_y = 1\n",
    "        elif category == 'Crime':\n",
    "            index_y = 2\n",
    "        elif category == 'Technology':\n",
    "            index_y = 3\n",
    "        elif category == 'Entertainment':\n",
    "            index_y = 4\n",
    "        elif category == 'Anti-Hillary':\n",
    "            index_y = 5\n",
    "        elif category == 'Fukushima':\n",
    "            index_y = 6\n",
    "        elif category == 'Foreign Countries':\n",
    "            index_y = 7\n",
    "        elif category == 'Trump Support':\n",
    "            index_y = 8\n",
    "        elif category == 'Gun Related':\n",
    "            index_y = 9\n",
    "        elif category == 'Video Games':\n",
    "            index_y = 10\n",
    "        elif category == 'Health':\n",
    "            index_y = 11\n",
    "        elif category == 'Business':\n",
    "            index_y = 12\n",
    "        elif category == 'Black Support':\n",
    "            index_y = 13\n",
    "        elif category == 'Anti-Islam':\n",
    "            index_y = 14\n",
    "        results.append(index_y)\n",
    "\n",
    "\n",
    "    return np.array(batches),np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "num_epochs = 4\n",
    "batch_size = 150\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "hidden_size = 100      # 1st layer and 2nd layer number of features\n",
    "input_size = total_words # Words in vocab\n",
    "num_classes = 15        # Categories: graphics, sci.space and baseball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "inputs = Variable(torch.randn(2, 5), requires_grad=True)\n",
    "target = Variable(torch.LongTensor(2).random_(5))\n",
    "output = loss(inputs, target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do Neural Network with Linear Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(input_size,hidden_size, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer_2 = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer_1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer_2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.output_layer(out)\n",
    "        return out\n",
    "    \n",
    "net = OurNet(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "# scheduler = ReduceLROnPlateau(optimizer, 'min') # for validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Step [100/4413], Loss: 1.0593\n",
      "Epoch [1/4], Step [200/4413], Loss: 1.1449\n",
      "Epoch [1/4], Step [300/4413], Loss: 0.8779\n",
      "Epoch [1/4], Step [400/4413], Loss: 0.8435\n",
      "Epoch [1/4], Step [500/4413], Loss: 0.6738\n",
      "Epoch [1/4], Step [600/4413], Loss: 0.7350\n",
      "Epoch [1/4], Step [700/4413], Loss: 0.9442\n",
      "Epoch [1/4], Step [800/4413], Loss: 0.8077\n",
      "Epoch [1/4], Step [900/4413], Loss: 0.8551\n",
      "Epoch [1/4], Step [1000/4413], Loss: 1.1576\n",
      "Epoch [1/4], Step [1100/4413], Loss: 0.8019\n",
      "Epoch [1/4], Step [1200/4413], Loss: 0.7339\n",
      "Epoch [1/4], Step [1300/4413], Loss: 0.9920\n",
      "Epoch [1/4], Step [1400/4413], Loss: 0.6972\n",
      "Epoch [1/4], Step [1500/4413], Loss: 0.7185\n",
      "Epoch [1/4], Step [1600/4413], Loss: 0.7443\n",
      "Epoch [1/4], Step [1700/4413], Loss: 0.7170\n",
      "Epoch [1/4], Step [1800/4413], Loss: 0.9561\n",
      "Epoch [1/4], Step [1900/4413], Loss: 0.7751\n",
      "Epoch [1/4], Step [2000/4413], Loss: 0.7682\n",
      "Epoch [1/4], Step [2100/4413], Loss: 0.8138\n",
      "Epoch [1/4], Step [2200/4413], Loss: 0.8708\n",
      "Epoch [1/4], Step [2300/4413], Loss: 0.7776\n",
      "Epoch [1/4], Step [2400/4413], Loss: 0.6362\n",
      "Epoch [1/4], Step [2500/4413], Loss: 0.7508\n",
      "Epoch [1/4], Step [2600/4413], Loss: 0.7599\n",
      "Epoch [1/4], Step [2700/4413], Loss: 0.8843\n",
      "Epoch [1/4], Step [2800/4413], Loss: 0.7592\n",
      "Epoch [1/4], Step [2900/4413], Loss: 0.7454\n",
      "Epoch [1/4], Step [3000/4413], Loss: 0.7633\n",
      "Epoch [1/4], Step [3100/4413], Loss: 0.7982\n",
      "Epoch [1/4], Step [3200/4413], Loss: 0.7472\n",
      "Epoch [1/4], Step [3300/4413], Loss: 0.6289\n",
      "Epoch [1/4], Step [3400/4413], Loss: 0.7287\n",
      "Epoch [1/4], Step [3500/4413], Loss: 0.7733\n",
      "Epoch [1/4], Step [3600/4413], Loss: 0.8470\n",
      "Epoch [1/4], Step [3700/4413], Loss: 0.8214\n",
      "Epoch [1/4], Step [3800/4413], Loss: 0.5782\n",
      "Epoch [1/4], Step [3900/4413], Loss: 0.6730\n",
      "Epoch [1/4], Step [4000/4413], Loss: 0.8231\n",
      "Epoch [1/4], Step [4100/4413], Loss: 0.6238\n",
      "Epoch [1/4], Step [4200/4413], Loss: 0.7828\n",
      "Epoch [1/4], Step [4300/4413], Loss: 0.8756\n",
      "Epoch [1/4], Step [4400/4413], Loss: 0.8354\n",
      "Epoch [2/4], Step [100/4413], Loss: 0.5636\n",
      "Epoch [2/4], Step [200/4413], Loss: 0.5964\n",
      "Epoch [2/4], Step [300/4413], Loss: 0.5180\n",
      "Epoch [2/4], Step [400/4413], Loss: 0.5264\n",
      "Epoch [2/4], Step [500/4413], Loss: 0.5197\n",
      "Epoch [2/4], Step [600/4413], Loss: 0.5584\n",
      "Epoch [2/4], Step [700/4413], Loss: 0.6466\n",
      "Epoch [2/4], Step [800/4413], Loss: 0.4794\n",
      "Epoch [2/4], Step [900/4413], Loss: 0.5337\n",
      "Epoch [2/4], Step [1000/4413], Loss: 0.6907\n",
      "Epoch [2/4], Step [1100/4413], Loss: 0.5831\n",
      "Epoch [2/4], Step [1200/4413], Loss: 0.5243\n",
      "Epoch [2/4], Step [1300/4413], Loss: 0.7731\n",
      "Epoch [2/4], Step [1400/4413], Loss: 0.4995\n",
      "Epoch [2/4], Step [1500/4413], Loss: 0.4382\n",
      "Epoch [2/4], Step [1600/4413], Loss: 0.4937\n",
      "Epoch [2/4], Step [1700/4413], Loss: 0.4823\n",
      "Epoch [2/4], Step [1800/4413], Loss: 0.5725\n",
      "Epoch [2/4], Step [1900/4413], Loss: 0.4892\n",
      "Epoch [2/4], Step [2000/4413], Loss: 0.5208\n",
      "Epoch [2/4], Step [2100/4413], Loss: 0.5785\n",
      "Epoch [2/4], Step [2200/4413], Loss: 0.6636\n",
      "Epoch [2/4], Step [2300/4413], Loss: 0.6202\n",
      "Epoch [2/4], Step [2400/4413], Loss: 0.5362\n",
      "Epoch [2/4], Step [2500/4413], Loss: 0.5202\n",
      "Epoch [2/4], Step [2600/4413], Loss: 0.4489\n",
      "Epoch [2/4], Step [2700/4413], Loss: 0.6216\n",
      "Epoch [2/4], Step [2800/4413], Loss: 0.5149\n",
      "Epoch [2/4], Step [2900/4413], Loss: 0.5257\n",
      "Epoch [2/4], Step [3000/4413], Loss: 0.5280\n",
      "Epoch [2/4], Step [3100/4413], Loss: 0.4904\n",
      "Epoch [2/4], Step [3200/4413], Loss: 0.5526\n",
      "Epoch [2/4], Step [3300/4413], Loss: 0.4226\n",
      "Epoch [2/4], Step [3400/4413], Loss: 0.4214\n",
      "Epoch [2/4], Step [3500/4413], Loss: 0.5725\n",
      "Epoch [2/4], Step [3600/4413], Loss: 0.6326\n",
      "Epoch [2/4], Step [3700/4413], Loss: 0.5863\n",
      "Epoch [2/4], Step [3800/4413], Loss: 0.4812\n",
      "Epoch [2/4], Step [3900/4413], Loss: 0.5159\n",
      "Epoch [2/4], Step [4000/4413], Loss: 0.6290\n",
      "Epoch [2/4], Step [4100/4413], Loss: 0.4780\n",
      "Epoch [2/4], Step [4200/4413], Loss: 0.5758\n",
      "Epoch [2/4], Step [4300/4413], Loss: 0.7358\n",
      "Epoch [2/4], Step [4400/4413], Loss: 0.6384\n",
      "Epoch [3/4], Step [100/4413], Loss: 0.4001\n",
      "Epoch [3/4], Step [200/4413], Loss: 0.4422\n",
      "Epoch [3/4], Step [300/4413], Loss: 0.3358\n",
      "Epoch [3/4], Step [400/4413], Loss: 0.4110\n",
      "Epoch [3/4], Step [500/4413], Loss: 0.3619\n",
      "Epoch [3/4], Step [600/4413], Loss: 0.4226\n",
      "Epoch [3/4], Step [700/4413], Loss: 0.4228\n",
      "Epoch [3/4], Step [800/4413], Loss: 0.3955\n",
      "Epoch [3/4], Step [900/4413], Loss: 0.4442\n",
      "Epoch [3/4], Step [1000/4413], Loss: 0.5869\n",
      "Epoch [3/4], Step [1100/4413], Loss: 0.4281\n",
      "Epoch [3/4], Step [1200/4413], Loss: 0.4354\n",
      "Epoch [3/4], Step [1300/4413], Loss: 0.7079\n",
      "Epoch [3/4], Step [1400/4413], Loss: 0.4213\n",
      "Epoch [3/4], Step [1500/4413], Loss: 0.3562\n",
      "Epoch [3/4], Step [1600/4413], Loss: 0.3881\n",
      "Epoch [3/4], Step [1700/4413], Loss: 0.3302\n",
      "Epoch [3/4], Step [1800/4413], Loss: 0.4158\n",
      "Epoch [3/4], Step [1900/4413], Loss: 0.4181\n",
      "Epoch [3/4], Step [2000/4413], Loss: 0.3855\n",
      "Epoch [3/4], Step [2100/4413], Loss: 0.4959\n",
      "Epoch [3/4], Step [2200/4413], Loss: 0.6104\n",
      "Epoch [3/4], Step [2300/4413], Loss: 0.5375\n",
      "Epoch [3/4], Step [2400/4413], Loss: 0.4562\n",
      "Epoch [3/4], Step [2500/4413], Loss: 0.4073\n",
      "Epoch [3/4], Step [2600/4413], Loss: 0.3801\n",
      "Epoch [3/4], Step [2700/4413], Loss: 0.4816\n",
      "Epoch [3/4], Step [2800/4413], Loss: 0.4355\n",
      "Epoch [3/4], Step [2900/4413], Loss: 0.4917\n",
      "Epoch [3/4], Step [3000/4413], Loss: 0.3709\n",
      "Epoch [3/4], Step [3100/4413], Loss: 0.3985\n",
      "Epoch [3/4], Step [3200/4413], Loss: 0.4480\n",
      "Epoch [3/4], Step [3300/4413], Loss: 0.3100\n",
      "Epoch [3/4], Step [3400/4413], Loss: 0.3269\n",
      "Epoch [3/4], Step [3500/4413], Loss: 0.4233\n",
      "Epoch [3/4], Step [3600/4413], Loss: 0.6071\n",
      "Epoch [3/4], Step [3700/4413], Loss: 0.5321\n",
      "Epoch [3/4], Step [3800/4413], Loss: 0.3943\n",
      "Epoch [3/4], Step [3900/4413], Loss: 0.4175\n",
      "Epoch [3/4], Step [4000/4413], Loss: 0.4449\n",
      "Epoch [3/4], Step [4100/4413], Loss: 0.3433\n",
      "Epoch [3/4], Step [4200/4413], Loss: 0.3817\n",
      "Epoch [3/4], Step [4300/4413], Loss: 0.6187\n",
      "Epoch [3/4], Step [4400/4413], Loss: 0.4656\n",
      "Epoch [4/4], Step [100/4413], Loss: 0.3856\n",
      "Epoch [4/4], Step [200/4413], Loss: 0.3898\n",
      "Epoch [4/4], Step [300/4413], Loss: 0.3137\n",
      "Epoch [4/4], Step [400/4413], Loss: 0.3083\n",
      "Epoch [4/4], Step [500/4413], Loss: 0.3403\n",
      "Epoch [4/4], Step [600/4413], Loss: 0.3240\n",
      "Epoch [4/4], Step [700/4413], Loss: 0.3146\n",
      "Epoch [4/4], Step [800/4413], Loss: 0.3088\n",
      "Epoch [4/4], Step [900/4413], Loss: 0.4007\n",
      "Epoch [4/4], Step [1000/4413], Loss: 0.5080\n",
      "Epoch [4/4], Step [1100/4413], Loss: 0.3432\n",
      "Epoch [4/4], Step [1200/4413], Loss: 0.3561\n",
      "Epoch [4/4], Step [1300/4413], Loss: 0.5798\n",
      "Epoch [4/4], Step [1400/4413], Loss: 0.3490\n",
      "Epoch [4/4], Step [1500/4413], Loss: 0.2606\n",
      "Epoch [4/4], Step [1600/4413], Loss: 0.3448\n",
      "Epoch [4/4], Step [1700/4413], Loss: 0.3201\n",
      "Epoch [4/4], Step [1800/4413], Loss: 0.3483\n",
      "Epoch [4/4], Step [1900/4413], Loss: 0.3809\n",
      "Epoch [4/4], Step [2000/4413], Loss: 0.3049\n",
      "Epoch [4/4], Step [2100/4413], Loss: 0.4336\n",
      "Epoch [4/4], Step [2200/4413], Loss: 0.4845\n",
      "Epoch [4/4], Step [2300/4413], Loss: 0.5224\n",
      "Epoch [4/4], Step [2400/4413], Loss: 0.3716\n",
      "Epoch [4/4], Step [2500/4413], Loss: 0.3760\n",
      "Epoch [4/4], Step [2600/4413], Loss: 0.3157\n",
      "Epoch [4/4], Step [2700/4413], Loss: 0.4103\n",
      "Epoch [4/4], Step [2800/4413], Loss: 0.4933\n",
      "Epoch [4/4], Step [2900/4413], Loss: 0.4915\n",
      "Epoch [4/4], Step [3000/4413], Loss: 0.3994\n",
      "Epoch [4/4], Step [3100/4413], Loss: 0.3182\n",
      "Epoch [4/4], Step [3200/4413], Loss: 0.4027\n",
      "Epoch [4/4], Step [3300/4413], Loss: 0.2973\n",
      "Epoch [4/4], Step [3400/4413], Loss: 0.2373\n",
      "Epoch [4/4], Step [3500/4413], Loss: 0.4101\n",
      "Epoch [4/4], Step [3600/4413], Loss: 0.5233\n",
      "Epoch [4/4], Step [3700/4413], Loss: 0.4051\n",
      "Epoch [4/4], Step [3800/4413], Loss: 0.3271\n",
      "Epoch [4/4], Step [3900/4413], Loss: 0.3727\n",
      "Epoch [4/4], Step [4000/4413], Loss: 0.4363\n",
      "Epoch [4/4], Step [4100/4413], Loss: 0.2821\n",
      "Epoch [4/4], Step [4200/4413], Loss: 0.3233\n",
      "Epoch [4/4], Step [4300/4413], Loss: 0.5731\n",
      "Epoch [4/4], Step [4400/4413], Loss: 0.3495\n"
     ]
    }
   ],
   "source": [
    "## Train the Model\n",
    "for epoch in range(num_epochs):\n",
    "    total_batch = int(len(filtered.tweet_text)/batch_size)\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "        batch_x,batch_y = get_batch(filtered,i,batch_size)\n",
    "        articles = Variable(torch.FloatTensor(batch_x))\n",
    "        labels = Variable(torch.LongTensor(batch_y))\n",
    "        #print(\"articles\",articles)\n",
    "        #print(batch_x, labels)\n",
    "        #print(\"size labels\",labels.size())\n",
    "\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()  # zero the gradient buffer\n",
    "        outputs = net(articles)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                   %(epoch+1, num_epochs, i+1, len(filtered.tweet_text)//batch_size, loss.data[0]))\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lemmatize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-362e92402843>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'I need to get healthy who wants to go to the gym'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Yesterday I saw a great football game'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lemmatize' is not defined"
     ]
    }
   ],
   "source": [
    "# lemmatize(['I need to get healthy who wants to go to the gym', 'Yesterday I saw a great football game'], min_length=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tensor([[ -2.1686,  -0.3003, -11.8906, -10.8411,   1.0247,  -5.6979, -20.9244,\n",
      "           2.4551,  18.9812, -24.1216, -24.0897, -25.0826,  -7.8877,  -3.5073,\n",
      "          -6.3526],\n",
      "        [ -3.6174,  -0.8996, -14.0025, -14.0994,   0.6767,  -7.0539, -26.6901,\n",
      "           2.9211,  26.0714, -31.7085, -31.5246, -32.6922, -10.4274,  -3.9084,\n",
      "          -8.3163]], grad_fn=<ThAddmmBackward>)\n",
      "tensor([8, 8])\n"
     ]
    }
   ],
   "source": [
    "test_set = filtered.loc[2150651].copy()\n",
    "\n",
    "test_set\n",
    "\n",
    "test_set.head()\n",
    "batch_x_test,batch_y_test = get_batch(test_set,0,2)\n",
    "articles = Variable(torch.FloatTensor(batch_x_test))\n",
    "labels = torch.LongTensor(batch_y_test)\n",
    "outputs = net(articles)\n",
    "print(outputs)\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "print(predicted)\n",
    "\n",
    "# torch.save(net.state_dict(), 'supervised_linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Convolutional Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(input_size, input_size, kernel_size=2) # dimensions need to be fixed\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=2)\n",
    "        self.mp = nn.MaxPool1d(2)\n",
    "        self.fc = nn.Linear(100, 15)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.mp(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.relu(self.mp(self.conv2(x)))\n",
    "        x = x.view(in_size, -1)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x)\n",
    "        \n",
    "convnet = ConvNet(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the Model\n",
    "for epoch in range(num_epochs):\n",
    "    total_batch = int(len(filtered.tweet_text)/batch_size)\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "        batch_x,batch_y = get_batch(filtered,i,batch_size)\n",
    "        articles = Variable(torch.FloatTensor(batch_x))\n",
    "        labels = Variable(torch.LongTensor(batch_y))\n",
    "        #print(\"articles\",articles)\n",
    "        #print(batch_x, labels)\n",
    "        #print(\"size labels\",labels.size())\n",
    "\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()  # zero the gradient buffer\n",
    "        outputs = convnet(articles)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                   %(epoch+1, num_epochs, i+1, len(filtered.tweet_text)//batch_size, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
