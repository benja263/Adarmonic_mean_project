{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model for text classification using a neural network to assign class probabilities\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from helpers import *\n",
    "# from tensorflow.contrib import learn\n",
    "#import torch\n",
    "#from torch.autograd import Variable\n",
    "#import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "#from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "# from gensim.parsing.preprocessing import remove_stopwords\n",
    "# import Pattern\n",
    "\n",
    "\n",
    "##### RUN FOLLOWING TO GENERATE THE CSV FILE WITH TWEET CONTENT AND HASHTAGS\n",
    "##### OTHERWISE LOAD THE SAVED DATAFRAME\n",
    "\n",
    "# data = pd.read_csv('../data/rus_troll_tweet_text.csv')\n",
    "# english_tweets = data[data.tweet_language == 'en']\n",
    "# hashtag_info = pd.read_csv('../data/rus_troll_tweet_stats.csv')[['tweetid', 'hashtags']]\n",
    "# merged = pd.merge(english_tweets, hashtag_info, on='tweetid')[['tweet_text', 'hashtags']]\n",
    "\n",
    "# # Remove tweets that have no hashtag\n",
    "# merged.replace('[]',np.nan, inplace=True)\n",
    "# merged.dropna(how='any', inplace=True)\n",
    "\n",
    "# # Remove hashtags and other tags and grammar stuff\n",
    "# data_clean=merged.replace('#\\w+', '', regex=True).replace('@\\w+', '', regex=True).replace('RT ', '', regex=True)\n",
    "# data_clean.tweet_text = data_clean.tweet_text.apply(lambda x: re.sub(r'http\\S+', '', x)).apply(lambda x: re.sub(r\"'|\\\"|`|:|\\?|~|,|\\.\", '', x))\n",
    "# data_clean.hashtags = data_clean.hashtags.apply(lambda x: x[1:-1]) # make list in hashtag column\n",
    "# duplicated = pd.DataFrame(data_clean.hashtags.str.split(', ').tolist(), index=data_clean.tweet_text).stack()\n",
    "# duplicated = duplicated.reset_index()[[0, 'tweet_text']] # var1 variable is currently labeled 0\n",
    "# duplicated.columns = ['hashtags', 'tweet_text'] # renaming var1\n",
    "\n",
    "# # Show top hashtags\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#     print(duplicated.hashtags.value_counts()[0:300])\n",
    "    \n",
    "# # Remove stop words\n",
    "# duplicated.tweet_text = duplicated.tweet_text.apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "\n",
    "# Filter dataframe for hashtags identified to be in a topic\n",
    "\n",
    "# Create list of relevant hashtags\n",
    "hashtag_list = ['MAGA', 'world', 'breaking', 'Breaking', 'TopNews', 'Breaking', 'money' 'WakeUpAmerica', 'MakeAmericaGreatAgain', \n",
    "                'ARRESTObama', 'TrumpTrain', 'pjnet', 'PJNET', 'Trump2016',\n",
    "                'maga', 'TrumpForPresident', 'TrumpBecause', 'ObamaLameDuck',\n",
    "                'ICelebrateTrumpWith', 'BlackLivesMatter', 'BlackTwitter', 'blacklivesmatter', \n",
    "                'BlackHistoryMonth', 'BlackSkinIsNotACrime',  'blacktolive', 'BlackToLive', 'blacktwitter',\n",
    "                'tech', 'science', 'entertainment', 'celebs', 'MyEmmyNominationWouldBe', 'soundcloud', 'iTunes',\n",
    "                'rap', 'RAP', 'showbiz', 'Music', 'life', 'health', 'sports', 'hockey', 'baseball', 'NowPlaying',\n",
    "                'Sports', 'MyOlympicSportWouldBe', 'ISIS', 'IslamKills',\n",
    "                'StopIslam', 'Hillary', 'ThingsMoreTrustedThanHillary', 'NeverHillary', 'CrookedHillary', 'LockHerUp',\n",
    "                'nukraine', 'Obama', 'ObamaWishlist',\n",
    "                'world', 'Syria', 'Turkey', 'Brussels', 'Russia', 'Iraq', 'business', 'news', 'local', \n",
    "                'Fukushima2015', 'FukushimaAgain', 'FUKUSHIMA2015', 'fukushima2015', 'IHatePokemonGoBecause',\n",
    "                'crime', 'PoliceBrutality', 'USFA', 'ArmyWomenProblems', '2A', 'NRA', 'BLM', 'tcot', 'TCOT', 'GOP',\n",
    "               'GOPDebate']\n",
    "\n",
    "# # Filter dataframe\n",
    "# filtered = duplicated[duplicated.hashtags.isin(hashtag_list)]\n",
    "\n",
    "\n",
    "# # Lemmatize the words and also keeping their context (verb, noun etc)\n",
    "\n",
    "# from gensim.utils import lemmatize\n",
    "# filtered.tweet_text = filtered.tweet_text.apply(lambda x: lemmatize(x, min_length=3))\n",
    "\n",
    "\n",
    "# # change hashtag names to the topic names\n",
    "# pd.options.mode.chained_assignment = None  # default='warn' \n",
    "\n",
    "# # General News\n",
    "# filtered.hashtags.loc[filtered[(filtered.hashtags == 'news') |\n",
    "#                                (filtered.hashtags == 'local')].index.values] = 'General News'\n",
    "\n",
    "# # Fukushima\n",
    "# filtered.hashtags.loc[filtered[(filtered.hashtags == 'Fukushima2015') |\n",
    "#                                (filtered.hashtags == 'FukushimaAgain') |\n",
    "#                               (filtered.hashtags == 'FUKUSHIMA2015') | \n",
    "#                               (filtered.hashtags == 'fukushima2015')].index.values] = 'Fukushima'\n",
    "\n",
    "# # Crime\n",
    "# filtered.hashtags.loc[filtered[(filtered.hashtags == 'crime') |\n",
    "#                                (filtered.hashtags == 'PoliceBrutality')].index.values] = 'Crime'\n",
    "\n",
    "# # Gun Related\n",
    "# filtered.hashtags.loc[filtered[(filtered.hashtags == 'USFA') |\n",
    "#                                (filtered.hashtags == 'ArmyWomenProblems')].index.values] = 'Gun Related'\n",
    "\n",
    "# # Anti-Islam\n",
    "# filtered.hashtags.loc[filtered[(filtered.hashtags == 'ISIS') |\n",
    "#                                (filtered.hashtags == 'IslamKills') |\n",
    "#                               (filtered.hashtags == 'StopIslam')].index.values] = 'Anti-Islam'\n",
    "\n",
    "# # Anti-Hillary\n",
    "# filtered.hashtags.loc[filtered[(filtered.hashtags == 'Hillary') |\n",
    "#                                (filtered.hashtags == 'ThingsMoreTrustedThanHillary') |\n",
    "#                               (filtered.hashtags == 'NeverHillary') |\n",
    "#                               (filtered.hashtags == 'CrookedHillary')].index.values] = 'Anti-Hillary'\n",
    "\n",
    "\n",
    "# # Foreign Countries\n",
    "# filtered.hashtags.loc[filtered[(filtered.hashtags == 'world') |\n",
    "#                                (filtered.hashtags == 'Syria') |\n",
    "#                               (filtered.hashtags == 'Turkey') |\n",
    "#                               (filtered.hashtags == 'Brussels') |\n",
    "#                               (filtered.hashtags == 'Russia') |\n",
    "#                               (filtered.hashtags == 'Iraq')].index.values] = 'Foreign Countries'\n",
    "\n",
    "# # Entertainment\n",
    "# filtered.hashtags.loc[filtered[(filtered.hashtags == 'entertainment') |\n",
    "#                                (filtered.hashtags == 'celebs') |\n",
    "#                               (filtered.hashtags == 'MyEmmyNominationWouldBe') |\n",
    "#                               (filtered.hashtags == 'soundcloud') |\n",
    "#                               (filtered.hashtags == 'iTunes') |\n",
    "#                               (filtered.hashtags == 'rap')].index.values] = 'Entertainment'\n",
    "\n",
    "\n",
    "# # Black Race Support\n",
    "# filtered.hashtags.loc[filtered[(filtered.hashtags == 'BlackLivesMatter') |\n",
    "#                                (filtered.hashtags == 'BlackTwitter') |\n",
    "#                               (filtered.hashtags == 'blacklivesmatter') |\n",
    "#                               (filtered.hashtags == 'BlackHistoryMonth') |\n",
    "#                               (filtered.hashtags == 'BlackSkinIsNotACrime') |\n",
    "#                               (filtered.hashtags == 'blacktolive') |\n",
    "#                               (filtered.hashtags == 'BlackToLive') |\n",
    "#                               (filtered.hashtags == 'blacktwitter')].index.values] = 'Black Support'\n",
    "\n",
    "# # Trump Support\n",
    "# filtered.hashtags.loc[filtered[(filtered.hashtags == 'MAGA') |\n",
    "#                                (filtered.hashtags == 'WakeUpAmerica') |\n",
    "#                               (filtered.hashtags == 'MakeAmericaGreatAgain') |\n",
    "#                               (filtered.hashtags == 'ARRESTObama') |\n",
    "#                               (filtered.hashtags == 'TrumpTrain') |\n",
    "#                               (filtered.hashtags == 'pjnet') |\n",
    "#                               (filtered.hashtags == 'PJNET') |\n",
    "#                               (filtered.hashtags == 'Trump2016') |\n",
    "#                               (filtered.hashtags == 'maga') |\n",
    "#                               (filtered.hashtags == 'TrumpForPresident') |\n",
    "#                               (filtered.hashtags == 'TrumpBecause') |\n",
    "#                               (filtered.hashtags == 'ObamaLameDuck') |\n",
    "#                               (filtered.hashtags == 'ICelebrateTrumpWith')].index.values] = 'Trump Support'\n",
    "\n",
    "\n",
    "# # Technology\n",
    "# filtered.hashtags.loc[filtered[(filtered.hashtags == 'tech') |\n",
    "#                                (filtered.hashtags == 'science')].index.values] = 'Technology'\n",
    "\n",
    "# # Health\n",
    "# filtered.hashtags.loc[filtered[filtered.hashtags == 'health'].index.values] = 'Health'\n",
    "\n",
    "# # Sports\n",
    "# filtered.hashtags.loc[filtered[(filtered.hashtags == 'sports') |\n",
    "#                                (filtered.hashtags == 'hockey') |\n",
    "#                               (filtered.hashtags == 'Sports') |\n",
    "#                               (filtered.hashtags == 'MyOlympicSportWouldBe')].index.values] = 'Sports'\n",
    "\n",
    "\n",
    "# # Business\n",
    "# filtered.hashtags.loc[filtered[filtered.hashtags == 'business'].index.values] = 'Business'\n",
    "\n",
    "# # Video GAmes\n",
    "# filtered.hashtags.loc[filtered[filtered.hashtags == 'IHatePokemonGoBecause'].index.values] = 'Video Games'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Twokenize -- a tokenizer designed for Twitter text in English and some other European languages.\n",
    "This tokenizer code has gone through a long history:\n",
    "(1) Brendan O'Connor wrote original version in Python, http://github.com/brendano/tweetmotif\n",
    "       TweetMotif: Exploratory Search and Topic Summarization for Twitter.\n",
    "       Brendan O'Connor, Michel Krieger, and David Ahn.\n",
    "       ICWSM-2010 (demo track), http://brenocon.com/oconnor_krieger_ahn.icwsm2010.tweetmotif.pdf\n",
    "(2a) Kevin Gimpel and Daniel Mills modified it for POS tagging for the CMU ARK Twitter POS Tagger\n",
    "(2b) Jason Baldridge and David Snyder ported it to Scala\n",
    "(3) Brendan bugfixed the Scala port and merged with POS-specific changes\n",
    "    for the CMU ARK Twitter POS Tagger\n",
    "(4) Tobi Owoputi ported it back to Java and added many improvements (2012-06)\n",
    "Current home is http://github.com/brendano/ark-tweet-nlp and http://www.ark.cs.cmu.edu/TweetNLP\n",
    "There have been at least 2 other Java ports, but they are not in the lineage for the code here.\n",
    "Ported to Python by Myle Ott <myleott@gmail.com>.\n",
    "\"\"\"\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import operator\n",
    "import re\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    from html.parser import HTMLParser\n",
    "except ImportError:\n",
    "    from HTMLParser import HTMLParser\n",
    "\n",
    "try:\n",
    "    import html\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "def regex_or(*items):\n",
    "    return '(?:' + '|'.join(items) + ')'\n",
    "\n",
    "Contractions = re.compile(u\"(?i)(\\w+)(n['â€™â€²]t|['â€™â€²]ve|['â€™â€²]ll|['â€™â€²]d|['â€™â€²]re|['â€™â€²]s|['â€™â€²]m)$\", re.UNICODE)\n",
    "Whitespace = re.compile(u\"[\\s\\u0020\\u00a0\\u1680\\u180e\\u202f\\u205f\\u3000\\u2000-\\u200a]+\", re.UNICODE)\n",
    "\n",
    "punctChars = r\"['\\\"â€œâ€â€˜â€™.?!â€¦,:;]\"\n",
    "#punctSeq   = punctChars+\"+\"\t#'anthem'. => ' anthem '.\n",
    "punctSeq   = r\"['\\\"â€œâ€â€˜â€™]+|[.?!,â€¦]+|[:;]+\"\t#'anthem'. => ' anthem ' .\n",
    "entity     = r\"&(?:amp|lt|gt|quot);\"\n",
    "#  URLs\n",
    "\n",
    "\n",
    "# BTO 2012-06: everyone thinks the daringfireball regex should be better, but they're wrong.\n",
    "# If you actually empirically test it the results are bad.\n",
    "# Please see https://github.com/brendano/ark-tweet-nlp/pull/9\n",
    "\n",
    "urlStart1  = r\"(?:https?://|\\bwww\\.)\"\n",
    "commonTLDs = r\"(?:com|org|edu|gov|net|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|pro|tel|travel|xxx)\"\n",
    "ccTLDs\t = r\"(?:ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|\" + \\\n",
    "r\"bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|\" + \\\n",
    "r\"er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|\" + \\\n",
    "r\"hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|\" + \\\n",
    "r\"lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|\" + \\\n",
    "r\"nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|sk|\" + \\\n",
    "r\"sl|sm|sn|so|sr|ss|st|su|sv|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|\" + \\\n",
    "r\"va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|za|zm|zw)\"\t#TODO: remove obscure country domains?\n",
    "urlStart2  = r\"\\b(?:[A-Za-z\\d-])+(?:\\.[A-Za-z0-9]+){0,3}\\.\" + regex_or(commonTLDs, ccTLDs) + r\"(?:\\.\"+ccTLDs+r\")?(?=\\W|$)\"\n",
    "urlBody    = r\"(?:[^\\.\\s<>][^\\s<>]*?)?\"\n",
    "urlExtraCrapBeforeEnd = regex_or(punctChars, entity) + \"+?\"\n",
    "urlEnd     = r\"(?:\\.\\.+|[<>]|\\s|$)\"\n",
    "url        = regex_or(urlStart1, urlStart2) + urlBody + \"(?=(?:\"+urlExtraCrapBeforeEnd+\")?\"+urlEnd+\")\"\n",
    "\n",
    "\n",
    "# Numeric\n",
    "timeLike   = r\"\\d+(?::\\d+){1,2}\"\n",
    "#numNum     = r\"\\d+\\.\\d+\"\n",
    "numberWithCommas = r\"(?:(?<!\\d)\\d{1,3},)+?\\d{3}\" + r\"(?=(?:[^,\\d]|$))\"\n",
    "numComb\t = u\"[\\u0024\\u058f\\u060b\\u09f2\\u09f3\\u09fb\\u0af1\\u0bf9\\u0e3f\\u17db\\ua838\\ufdfc\\ufe69\\uff04\\uffe0\\uffe1\\uffe5\\uffe6\\u00a2-\\u00a5\\u20a0-\\u20b9]?\\\\d+(?:\\\\.\\\\d+)+%?\"\n",
    "\n",
    "# Abbreviations\n",
    "boundaryNotDot = regex_or(\"$\", r\"\\s\", r\"[â€œ\\\"?!,:;]\", entity)\n",
    "aa1  = r\"(?:[A-Za-z]\\.){2,}(?=\" + boundaryNotDot + \")\"\n",
    "aa2  = r\"[^A-Za-z](?:[A-Za-z]\\.){1,}[A-Za-z](?=\" + boundaryNotDot + \")\"\n",
    "standardAbbreviations = r\"\\b(?:[Mm]r|[Mm]rs|[Mm]s|[Dd]r|[Ss]r|[Jj]r|[Rr]ep|[Ss]en|[Ss]t)\\.\"\n",
    "arbitraryAbbrev = regex_or(aa1, aa2, standardAbbreviations)\n",
    "separators  = \"(?:--+|â€•|â€”|~|â€“|=)\"\n",
    "decorations = u\"(?:[â™«â™ª]+|[â˜…â˜†]+|[â™¥â¤â™¡]+|[\\u2639-\\u263b]+|[\\ue001-\\uebbb]+)\"\n",
    "thingsThatSplitWords = r\"[^\\s\\.,?\\\"]\"\n",
    "embeddedApostrophe = thingsThatSplitWords+r\"+['â€™â€²]\" + thingsThatSplitWords + \"*\"\n",
    "\n",
    "#  Emoticons\n",
    "# myleott: in Python the (?iu) flags affect the whole expression\n",
    "#normalEyes = \"(?iu)[:=]\" # 8 and x are eyes but cause problems\n",
    "normalEyes = \"[:=]\" # 8 and x are eyes but cause problems\n",
    "wink = \"[;]\"\n",
    "noseArea = \"(?:|-|[^a-zA-Z0-9 ])\" # doesn't get ðŸ˜¢\n",
    "happyMouths = r\"[D\\)\\]\\}]+\"\n",
    "sadMouths = r\"[\\(\\[\\{]+\"\n",
    "tongue = \"[pPd3]+\"\n",
    "otherMouths = r\"(?:[oO]+|[/\\\\]+|[vV]+|[Ss]+|[|]+)\" # remove forward slash if http://'s aren't cleaned\n",
    "\n",
    "# mouth repetition examples:\n",
    "# @aliciakeys Put it in a love song :-))\n",
    "# @hellocalyclops =))=))=)) Oh well\n",
    "\n",
    "# myleott: try to be as case insensitive as possible, but still not perfect, e.g., o.O fails\n",
    "#bfLeft = u\"(â™¥|0|o|Â°|v|\\\\$|t|x|;|\\u0ca0|@|Ê˜|â€¢|ãƒ»|â—•|\\\\^|Â¬|\\\\*)\".encode('utf-8')\n",
    "bfLeft = u\"(â™¥|0|[oO]|Â°|[vV]|\\\\$|[tT]|[xX]|;|\\u0ca0|@|Ê˜|â€¢|ãƒ»|â—•|\\\\^|Â¬|\\\\*)\"\n",
    "bfCenter = r\"(?:[\\.]|[_-]+)\"\n",
    "bfRight = r\"\\2\"\n",
    "s3 = r\"(?:--['\\\"])\"\n",
    "s4 = r\"(?:<|&lt;|>|&gt;)[\\._-]+(?:<|&lt;|>|&gt;)\"\n",
    "s5 = \"(?:[.][_]+[.])\"\n",
    "# myleott: in Python the (?i) flag affects the whole expression\n",
    "#basicface = \"(?:(?i)\" +bfLeft+bfCenter+bfRight+ \")|\" +s3+ \"|\" +s4+ \"|\" + s5\n",
    "basicface = \"(?:\" +bfLeft+bfCenter+bfRight+ \")|\" +s3+ \"|\" +s4+ \"|\" + s5\n",
    "\n",
    "eeLeft = r\"[ï¼¼\\\\ÆªÔ„\\(ï¼ˆ<>;ãƒ½\\-=~\\*]+\"\n",
    "eeRight= u\"[\\\\-=\\\\);'\\u0022<>Êƒï¼‰/ï¼ãƒŽï¾‰ä¸¿â•¯Ïƒã£Âµ~\\\\*]+\"\n",
    "eeSymbol = r\"[^A-Za-z0-9\\s\\(\\)\\*:=-]\"\n",
    "eastEmote = eeLeft + \"(?:\"+basicface+\"|\" +eeSymbol+\")+\" + eeRight\n",
    "\n",
    "oOEmote = r\"(?:[oO]\" + bfCenter + r\"[oO])\"\n",
    "\n",
    "\n",
    "emoticon = regex_or(\n",
    "        # Standard version  :) :( :] :D :P\n",
    "        \"(?:>|&gt;)?\" + regex_or(normalEyes, wink) + regex_or(noseArea,\"[Oo]\") + regex_or(tongue+r\"(?=\\W|$|RT|rt|Rt)\", otherMouths+r\"(?=\\W|$|RT|rt|Rt)\", sadMouths, happyMouths),\n",
    "\n",
    "        # reversed version (: D:  use positive lookbehind to remove \"(word):\"\n",
    "        # because eyes on the right side is more ambiguous with the standard usage of : ;\n",
    "        regex_or(\"(?<=(?: ))\", \"(?<=(?:^))\") + regex_or(sadMouths,happyMouths,otherMouths) + noseArea + regex_or(normalEyes, wink) + \"(?:<|&lt;)?\",\n",
    "\n",
    "        #inspired by http://en.wikipedia.org/wiki/User:Scapler/emoticons#East_Asian_style\n",
    "        eastEmote.replace(\"2\", \"1\", 1), basicface,\n",
    "        # iOS 'emoji' characters (some smileys, some symbols) [\\ue001-\\uebbb]\n",
    "        # TODO should try a big precompiled lexicon from Wikipedia, Dan Ramage told me (BTO) he does this\n",
    "\n",
    "        # myleott: o.O and O.o are two of the biggest sources of differences\n",
    "        #          between this and the Java version. One little hack won't hurt...\n",
    "        oOEmote\n",
    ")\n",
    "\n",
    "Hearts = \"(?:<+/?3+)+\" #the other hearts are in decorations\n",
    "\n",
    "Arrows = regex_or(r\"(?:<*[-â€•â€”=]*>+|<+[-â€•â€”=]*>*)\", u\"[\\u2190-\\u21ff]+\")\n",
    "\n",
    "# BTO 2011-06: restored Hashtag, AtMention protection (dropped in original scala port) because it fixes\n",
    "# \"hello (#hashtag)\" ==> \"hello (#hashtag )\"  WRONG\n",
    "# \"hello (#hashtag)\" ==> \"hello ( #hashtag )\"  RIGHT\n",
    "# \"hello (@person)\" ==> \"hello (@person )\"  WRONG\n",
    "# \"hello (@person)\" ==> \"hello ( @person )\"  RIGHT\n",
    "# ... Some sort of weird interaction with edgepunct I guess, because edgepunct\n",
    "# has poor content-symbol detection.\n",
    "\n",
    "# This also gets #1 #40 which probably aren't hashtags .. but good as tokens.\n",
    "# If you want good hashtag identification, use a different regex.\n",
    "Hashtag = \"#[a-zA-Z0-9_]+\"  #optional: lookbehind for \\b\n",
    "#optional: lookbehind for \\b, max length 15\n",
    "AtMention = \"[@ï¼ ][a-zA-Z0-9_]+\"\n",
    "\n",
    "# I was worried this would conflict with at-mentions\n",
    "# but seems ok in sample of 5800: 7 changes all email fixes\n",
    "# http://www.regular-expressions.info/email.html\n",
    "Bound = r\"(?:\\W|^|$)\"\n",
    "Email = regex_or(\"(?<=(?:\\W))\", \"(?<=(?:^))\") + r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,4}(?=\" +Bound+\")\"\n",
    "\n",
    "# We will be tokenizing using these regexps as delimiters\n",
    "# Additionally, these things are \"protected\", meaning they shouldn't be further split themselves.\n",
    "Protected  = re.compile(\n",
    "    regex_or(\n",
    "        Hearts,\n",
    "        url,\n",
    "        Email,\n",
    "        timeLike,\n",
    "        #numNum,\n",
    "        numberWithCommas,\n",
    "        numComb,\n",
    "        emoticon,\n",
    "        Arrows,\n",
    "        entity,\n",
    "        punctSeq,\n",
    "        arbitraryAbbrev,\n",
    "        separators,\n",
    "        decorations,\n",
    "        embeddedApostrophe,\n",
    "        Hashtag,\n",
    "        AtMention), re.UNICODE)\n",
    "\n",
    "# Edge punctuation\n",
    "# Want: 'foo' => ' foo '\n",
    "# While also:   don't => don't\n",
    "# the first is considered \"edge punctuation\".\n",
    "# the second is word-internal punctuation -- don't want to mess with it.\n",
    "# BTO (2011-06): the edgepunct system seems to be the #1 source of problems these days.\n",
    "# I remember it causing lots of trouble in the past as well.  Would be good to revisit or eliminate.\n",
    "\n",
    "# Note the 'smart quotes' (http://en.wikipedia.org/wiki/Smart_quotes)\n",
    "#edgePunctChars    = r\"'\\\"â€œâ€â€˜â€™Â«Â»{}\\(\\)\\[\\]\\*&\" #add \\\\p{So}? (symbols)\n",
    "edgePunctChars    = u\"'\\\"â€œâ€â€˜â€™Â«Â»{}\\\\(\\\\)\\\\[\\\\]\\\\*&\" #add \\\\p{So}? (symbols)\n",
    "edgePunct    = \"[\" + edgePunctChars + \"]\"\n",
    "notEdgePunct = \"[a-zA-Z0-9]\" # content characters\n",
    "offEdge = r\"(^|$|:|;|\\s|\\.|,)\"  # colon here gets \"(hello):\" ==> \"( hello ):\"\n",
    "EdgePunctLeft  = re.compile(offEdge + \"(\"+edgePunct+\"+)(\"+notEdgePunct+\")\", re.UNICODE)\n",
    "EdgePunctRight = re.compile(\"(\"+notEdgePunct+\")(\"+edgePunct+\"+)\" + offEdge, re.UNICODE)\n",
    "\n",
    "def splitEdgePunct(input):\n",
    "    input = EdgePunctLeft.sub(r\"\\1\\2 \\3\", input)\n",
    "    input = EdgePunctRight.sub(r\"\\1 \\2\\3\", input)\n",
    "    return input\n",
    "\n",
    "# The main work of tokenizing a tweet.\n",
    "def simpleTokenize(text):\n",
    "\n",
    "    # Do the no-brainers first\n",
    "    splitPunctText = splitEdgePunct(text)\n",
    "\n",
    "    textLength = len(splitPunctText)\n",
    "\n",
    "    # BTO: the logic here got quite convoluted via the Scala porting detour\n",
    "    # It would be good to switch back to a nice simple procedural style like in the Python version\n",
    "    # ... Scala is such a pain.  Never again.\n",
    "\n",
    "    # Find the matches for subsequences that should be protected,\n",
    "    # e.g. URLs, 1.0, U.N.K.L.E., 12:53\n",
    "    bads = []\n",
    "    badSpans = []\n",
    "    for match in Protected.finditer(splitPunctText):\n",
    "        # The spans of the \"bads\" should not be split.\n",
    "        if (match.start() != match.end()): #unnecessary?\n",
    "            bads.append( [splitPunctText[match.start():match.end()]] )\n",
    "            badSpans.append( (match.start(), match.end()) )\n",
    "\n",
    "    # Create a list of indices to create the \"goods\", which can be\n",
    "    # split. We are taking \"bad\" spans like\n",
    "    #     List((2,5), (8,10))\n",
    "    # to create\n",
    "    #     List(0, 2, 5, 8, 10, 12)\n",
    "    # where, e.g., \"12\" here would be the textLength\n",
    "    # has an even length and no indices are the same\n",
    "    indices = [0]\n",
    "    for (first, second) in badSpans:\n",
    "        indices.append(first)\n",
    "        indices.append(second)\n",
    "    indices.append(textLength)\n",
    "\n",
    "    # Group the indices and map them to their respective portion of the string\n",
    "    splitGoods = []\n",
    "    for i in range(0, len(indices), 2):\n",
    "        goodstr = splitPunctText[indices[i]:indices[i+1]]\n",
    "        splitstr = goodstr.strip().split(\" \")\n",
    "        splitGoods.append(splitstr)\n",
    "\n",
    "    #  Reinterpolate the 'good' and 'bad' Lists, ensuring that\n",
    "    #  additonal tokens from last good item get included\n",
    "    zippedStr = []\n",
    "    for i in range(len(bads)):\n",
    "        zippedStr = addAllnonempty(zippedStr, splitGoods[i])\n",
    "        zippedStr = addAllnonempty(zippedStr, bads[i])\n",
    "    zippedStr = addAllnonempty(zippedStr, splitGoods[len(bads)])\n",
    "\n",
    "    # BTO: our POS tagger wants \"ur\" and \"you're\" to both be one token.\n",
    "    # Uncomment to get \"you 're\"\n",
    "    #splitStr = []\n",
    "    #for tok in zippedStr:\n",
    "    #    splitStr.extend(splitToken(tok))\n",
    "    #zippedStr = splitStr\n",
    "\n",
    "    return zippedStr\n",
    "\n",
    "def addAllnonempty(master, smaller):\n",
    "    for s in smaller:\n",
    "        strim = s.strip()\n",
    "        if (len(strim) > 0):\n",
    "            master.append(strim)\n",
    "    return master\n",
    "\n",
    "# \"foo   bar \" => \"foo bar\"\n",
    "def squeezeWhitespace(input):\n",
    "    return Whitespace.sub(\" \", input).strip()\n",
    "\n",
    "# Final pass tokenization based on special patterns\n",
    "def splitToken(token):\n",
    "    m = Contractions.search(token)\n",
    "    if m:\n",
    "        return [m.group(1), m.group(2)]\n",
    "    return [token]\n",
    "\n",
    "# Assume 'text' has no HTML escaping.\n",
    "def tokenize(text):\n",
    "    return simpleTokenize(squeezeWhitespace(text))\n",
    "\n",
    "\n",
    "# Twitter text comes HTML-escaped, so unescape it.\n",
    "# We also first unescape &amp;'s, in case the text has been buggily double-escaped.\n",
    "def normalizeTextForTagger(text):\n",
    "    assert sys.version_info[0] >= 3 and sys.version_info[1] > 3, 'Python version >3.3 required'\n",
    "    text = text.replace(\"&amp;\", \"&\")\n",
    "    text = html.unescape(text)\n",
    "    return text\n",
    "\n",
    "# This is intended for raw tweet text -- we do some HTML entity unescaping before running the tagger.\n",
    "#\n",
    "# This function normalizes the input text BEFORE calling the tokenizer.\n",
    "# So the tokens you get back may not exactly correspond to\n",
    "# substrings of the original text.\n",
    "def tokenizeRawTweetText(text):\n",
    "    tokens = tokenize(normalizeTextForTagger(text))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = filtered[(filtered.hashtags == 'Sports') |\n",
    "               (filtered.hashtags == 'Health') |\n",
    "                (filtered.hashtags == 'Trump Support') |\n",
    "                (filtered.hashtags == 'Black Support') |\n",
    "                (filtered.hashtags == 'Entertainment') |\n",
    "                (filtered.hashtags == 'Foreign Countries') |\n",
    "                (filtered.hashtags == 'Anti-Trump') |\n",
    "                (filtered.hashtags == 'Health') |\n",
    "                (filtered.hashtags == 'Anti-Islam') |\n",
    "                (filtered.hashtags == 'Patriot') |\n",
    "                (filtered.hashtags == 'Crime') |\n",
    "                (filtered.hashtags == 'Fukushima') |\n",
    "                (filtered.hashtags == 'News')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords2 = open(access_folder() + 'morestopwords.txt', \"r\").read().split('\\n')\n",
    "stopwords2 = list(filter(None, stopwords2)) # fastest\n",
    "stopwords2 = [word for word in stopwords2 if word[0]!='#' ]\n",
    "stopwords3 = open(access_folder() + 'common-english-prep-conj.txt' , \"r\").read().split(',')\n",
    "stopwords4 = open(access_folder() + 'common-english-words.txt' , \"r\").read().split(',')\n",
    "\n",
    "stopwords = []\n",
    "stopwords = stopwords + stopwords2 +  stopwords3 + stopwords4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load english dataframe\n",
    "filtered = pd.read_csv('../generated/tweets_n_hashtags.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>news</td>\n",
       "      <td>['john', 'carroll', 'university', 'gets', '$22...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>local</td>\n",
       "      <td>['spring', 'cook', 'book', 'healthy', 'chocola...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>news</td>\n",
       "      <td>['rocky', 'river', 'prepares', 'more', 'floodi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>news</td>\n",
       "      <td>['forecast', 'cooler', 'mid-week', 'cooler', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>TopNews</td>\n",
       "      <td>['brothers', 'farook', 'one', 'decorated', 've...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hashtags                                         tweet_text\n",
       "33     news  ['john', 'carroll', 'university', 'gets', '$22...\n",
       "34    local  ['spring', 'cook', 'book', 'healthy', 'chocola...\n",
       "37     news  ['rocky', 'river', 'prepares', 'more', 'floodi...\n",
       "39     news  ['forecast', 'cooler', 'mid-week', 'cooler', '...\n",
       "40  TopNews  ['brothers', 'farook', 'one', 'decorated', 've..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered.tweet_text = filtered.tweet_text.apply(lambda x: x[1:-1]).replace(\"b'\", '', regex=True).replace(\"'\", '', regex=True)\n",
    "filtered.tweet_text = filtered.tweet_text.apply(lambda x: x.split(', '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# News\n",
    "filtered.hashtags.loc[filtered[(filtered.hashtags == 'news') |\n",
    "                               (filtered.hashtags == 'local') |\n",
    "                               (filtered.hashtags == 'isis') |\n",
    "                               (filtered.hashtags == 'world') |\n",
    "                               (filtered.hashtags == 'breaking') |\n",
    "                               (filtered.hashtags == 'business') |\n",
    "                               (filtered.hashtags == 'TopNews') |\n",
    "                               (filtered.hashtags == 'Breaking') |\n",
    "                               (filtered.hashtags == 'money')].index.values] = 'News'\n",
    "\n",
    "# Fukushima\n",
    "filtered.hashtags.loc[filtered[(filtered.hashtags == 'Fukushima2015') |\n",
    "                               (filtered.hashtags == 'FukushimaAgain') |\n",
    "                              (filtered.hashtags == 'FUKUSHIMA2015') | \n",
    "                              (filtered.hashtags == 'fukushima2015')].index.values] = 'Fukushima'\n",
    "\n",
    "# Crime\n",
    "filtered.hashtags.loc[filtered[(filtered.hashtags == 'crime') |\n",
    "                               (filtered.hashtags == 'PoliceBrutality')].index.values] = 'Crime'\n",
    "\n",
    "# Patriot\n",
    "filtered.hashtags.loc[filtered[(filtered.hashtags == 'NRA') |\n",
    "                               (filtered.hashtags == 'USFA') |\n",
    "                               (filtered.hashtags == 'NRA') |\n",
    "                               (filtered.hashtags == 'ArmyWomenProblems') |\n",
    "                              (filtered.hashtags == 'pjnet') |\n",
    "                              (filtered.hashtags == 'PJNET') |\n",
    "                              (filtered.hashtags == '2A')].index.values] = 'Patriot'\n",
    "\n",
    "\n",
    "# Anti-Islam\n",
    "filtered.hashtags.loc[filtered[(filtered.hashtags == 'ISIS') |\n",
    "                               (filtered.hashtags == 'IslamKills') |\n",
    "                              (filtered.hashtags == 'StopIslam')].index.values] = 'Anti-Islam'\n",
    "\n",
    "# Anti-Trump\n",
    "filtered.hashtags.loc[filtered[(filtered.hashtags == 'Hillary') |\n",
    "                               (filtered.hashtags == 'ThingsMoreTrustedThanHillary') |\n",
    "                              (filtered.hashtags == 'NeverHillary') |\n",
    "                               (filtered.hashtags == 'ARRESTObama') |\n",
    "                               (filtered.hashtags == 'LockHerUp') |\n",
    "                               (filtered.hashtags == 'ObamaWishlist') |\n",
    "                               (filtered.hashtags == 'Obama') |\n",
    "                              (filtered.hashtags == 'ObamaLameDuck') |\n",
    "                              (filtered.hashtags == 'CrookedHillary')].index.values] = 'Anti-Trump'\n",
    "\n",
    "\n",
    "# Foreign Countries\n",
    "filtered.hashtags.loc[filtered[(filtered.hashtags == 'isis') |\n",
    "                               (filtered.hashtags == 'Syria') |\n",
    "                              (filtered.hashtags == 'Turkey') |\n",
    "                               (filtered.hashtags == 'nukraine') |\n",
    "                              (filtered.hashtags == 'Brussels') |\n",
    "                              (filtered.hashtags == 'Russia') |\n",
    "                              (filtered.hashtags == 'Iraq')].index.values] = 'Foreign Countries'\n",
    "\n",
    "# Entertainment\n",
    "filtered.hashtags.loc[filtered[(filtered.hashtags == 'entertainment') |\n",
    "                               (filtered.hashtags == 'celebs') |\n",
    "                               (filtered.hashtags == 'NowPlaying') |\n",
    "                               (filtered.hashtags == 'Showbiz') |\n",
    "                               (filtered.hashtags == 'showbiz') |\n",
    "                               (filtered.hashtags == 'RAP') |\n",
    "                               (filtered.hashtags == 'life') |\n",
    "                               (filtered.hashtags == 'IHatePokemonGoBecause') |\n",
    "                               (filtered.hashtags == 'Music') |\n",
    "                              (filtered.hashtags == 'MyEmmyNominationWouldBe') |\n",
    "                              (filtered.hashtags == 'soundcloud') |\n",
    "                              (filtered.hashtags == 'iTunes') |\n",
    "                              (filtered.hashtags == 'rap')].index.values] = 'Entertainment'\n",
    "\n",
    "\n",
    "# Black Race Support\n",
    "filtered.hashtags.loc[filtered[(filtered.hashtags == 'BlackLivesMatter') |\n",
    "                               (filtered.hashtags == 'BlackTwitter') |\n",
    "                               (filtered.hashtags == 'blacktolive') |\n",
    "                              (filtered.hashtags == 'blacklivesmatter') |\n",
    "                              (filtered.hashtags == 'BlackHistoryMonth') |\n",
    "                              (filtered.hashtags == 'BlackSkinIsNotACrime') |\n",
    "                              (filtered.hashtags == 'BLM') |\n",
    "                              (filtered.hashtags == 'BlackToLive') |\n",
    "                              (filtered.hashtags == 'blacktwitter')].index.values] = 'Black Support'\n",
    "\n",
    "# Trump Support\n",
    "filtered.hashtags.loc[filtered[(filtered.hashtags == 'MAGA') |\n",
    "                               (filtered.hashtags == 'WakeUpAmerica') |\n",
    "                              (filtered.hashtags == 'MakeAmericaGreatAgain') |\n",
    "                              (filtered.hashtags == 'TrumpTrain') |\n",
    "                              (filtered.hashtags == 'Trump2016') |\n",
    "                              (filtered.hashtags == 'maga') |\n",
    "                               (filtered.hashtags == 'tcot') |\n",
    "                               (filtered.hashtags == 'TCOT') |\n",
    "                               (filtered.hashtags == 'GOP') |\n",
    "                               (filtered.hashtags == 'GOPDebate') |\n",
    "                               (filtered.hashtags == 'ccot') |\n",
    "                               (filtered.hashtags == 'AmericaFirst') |\n",
    "                              (filtered.hashtags == 'TrumpForPresident') |\n",
    "                              (filtered.hashtags == 'TrumpBecause') |\n",
    "                              (filtered.hashtags == 'ICelebrateTrumpWith')].index.values] = 'Trump Support'\n",
    "\n",
    "\n",
    "# Health\n",
    "filtered.hashtags.loc[filtered[(filtered.hashtags == 'health') |\n",
    "                              (filtered.hashtags == 'environment')].index.values] = 'Health'\n",
    "\n",
    "# Sports\n",
    "filtered.hashtags.loc[filtered[(filtered.hashtags == 'sports') |\n",
    "                               (filtered.hashtags == 'hockey') |\n",
    "                               (filtered.hashtags == 'Hockey') |\n",
    "                               (filtered.hashtags == 'baseball') |\n",
    "                              (filtered.hashtags == 'Sports') |\n",
    "                              (filtered.hashtags == 'MyOlympicSportWouldBe')].index.values] = 'Sports'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = filtered[filtered.hashtags != 'science']\n",
    "filtered = filtered[filtered.hashtags != 'tech']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['News', 'Sports', 'Crime', 'Entertainment', 'Anti-Trump',\n",
       "       'Fukushima', 'Trump Support', 'Patriot', 'Health', 'Black Support',\n",
       "       'Anti-Islam', 'Foreign Countries'], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered.hashtags.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered.tweet_text = filtered.tweet_text.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import lemmatize\n",
    "filtered.tweet_text = filtered.tweet_text.apply(lambda x: lemmatize(x, min_length=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_list = []\n",
    "for tweet in filtered.tweet_text.values:\n",
    "    sentence = []\n",
    "    for word in tweet:\n",
    "        sentence.append(word[0:-3].decode(\"utf-8\"))\n",
    "    tweet_list.append(sentence)\n",
    "\n",
    "filtered.tweet_text = tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered.to_csv('tweets_n_hashtags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['News', 'Sports', 'Crime', 'Fukushima', 'Entertainment', \n",
    "              'Anti-Trump', 'Patriot', 'Trump Support',\n",
    "              'Foreign Countries', 'Health',\n",
    "              'Black Support', 'Anti-Islam']\n",
    "from collections import Counter\n",
    "\n",
    "vocab = Counter()\n",
    "\n",
    "for text in filtered.tweet_text:\n",
    "    for word in text: ## change this because the texts are already a list of words\n",
    "        vocab[word[2:-1].lower()]+=1\n",
    "\n",
    "total_words = len(vocab)\n",
    "\n",
    "def get_word_2_index(vocab):\n",
    "    word2index = {}\n",
    "    for i,word in enumerate(vocab):\n",
    "        word2index[word.lower()] = i\n",
    "\n",
    "    return word2index\n",
    "\n",
    "word2index = get_word_2_index(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35149"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>News</td>\n",
       "      <td>[john, carroll, university, get, gift]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>News</td>\n",
       "      <td>[spring, cook, book, healthy, chocolate, bar]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>News</td>\n",
       "      <td>[rocky, river, prepare, more, flooding, city, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>News</td>\n",
       "      <td>[forecast, cooler, mid, week, cooler, isolated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>News</td>\n",
       "      <td>[brother, farook, decorate, veteran, killer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>News</td>\n",
       "      <td>[arrest, connection, art, festival, theft, arr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>News</td>\n",
       "      <td>[director, craven, die, scream, filmmaker, die...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Sports</td>\n",
       "      <td>[secretariat, dominate, american, pharoah, bel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>News</td>\n",
       "      <td>[black, box, miss, german, airline, crash]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Sports</td>\n",
       "      <td>[dale, earnhardt, get, engage, girlfriend, amy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Sports</td>\n",
       "      <td>[brown, mingo, undergo, surgery, brown, lineba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>News</td>\n",
       "      <td>[avenger, explode, thursday]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Crime</td>\n",
       "      <td>[clean, sweep, vacuum, cleaner, thief, mayfiel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>News</td>\n",
       "      <td>[state, senate, expect, vote, common, core, bill]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Sports</td>\n",
       "      <td>[johns, take, trust, new, report, look, nfls, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Crime</td>\n",
       "      <td>[cleveland, height, police, look, man, shot, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Crime</td>\n",
       "      <td>[medina, man, use, trash, hide, smell, daughte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Entertainment</td>\n",
       "      <td>[best, cast, director]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Entertainment</td>\n",
       "      <td>[take, hand, add, plan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Anti-Trump</td>\n",
       "      <td>[voting, voting, gop, nominee, vote, htt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Fukushima</td>\n",
       "      <td>[god, help, sound, absurd]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Fukushima</td>\n",
       "      <td>[thermal, nuclear, power, plant, ukraine, inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>News</td>\n",
       "      <td>[richard, kivett, elected, mayor, village, sun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>News</td>\n",
       "      <td>[yeman, fighter, take, aden, tawahi, district]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>News</td>\n",
       "      <td>[yeman, fighter, take, aden, tawahi, district]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>News</td>\n",
       "      <td>[agriculture, equestrian, center, take, shape,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>News</td>\n",
       "      <td>[agriculture, equestrian, center, take, shape,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>News</td>\n",
       "      <td>[arrest, polouse, drive, stolen, vehicle, dwi,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>News</td>\n",
       "      <td>[saint, patriot, joint, practice, bring, chang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>News</td>\n",
       "      <td>[clinton, campaign, arrest, make]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150486</th>\n",
       "      <td>Black Support</td>\n",
       "      <td>[common, color, blue, youre, blackthen, know]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150488</th>\n",
       "      <td>Trump Support</td>\n",
       "      <td>[worst, america, guess, hate, half, american]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150489</th>\n",
       "      <td>Anti-Trump</td>\n",
       "      <td>[worst, america, guess, hate, half, american]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150518</th>\n",
       "      <td>Sports</td>\n",
       "      <td>[fangirling, try, scream, time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150520</th>\n",
       "      <td>Trump Support</td>\n",
       "      <td>[president, put, safety, american, first]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150558</th>\n",
       "      <td>Patriot</td>\n",
       "      <td>[hold, hand, battle, cuddle, airstrike]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150564</th>\n",
       "      <td>Patriot</td>\n",
       "      <td>[divorce, man, friendly, fire]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150570</th>\n",
       "      <td>Sports</td>\n",
       "      <td>[fangirling, try, scream, time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150586</th>\n",
       "      <td>Trump Support</td>\n",
       "      <td>[truth, longer, allow, british, cop, fire, cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150603</th>\n",
       "      <td>Black Support</td>\n",
       "      <td>[victory, fight, isnt, good, momentum]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150606</th>\n",
       "      <td>Trump Support</td>\n",
       "      <td>[remember, take, salary, accept, third, paycheck]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150614</th>\n",
       "      <td>News</td>\n",
       "      <td>[refuse, reveal, source, wont, seem, confident]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150626</th>\n",
       "      <td>Patriot</td>\n",
       "      <td>[allow, carry, gun, vagina]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150638</th>\n",
       "      <td>Patriot</td>\n",
       "      <td>[male, actually, work]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150651</th>\n",
       "      <td>Patriot</td>\n",
       "      <td>[get, bullet, bullet, man, get]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150667</th>\n",
       "      <td>Anti-Trump</td>\n",
       "      <td>[come, really]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150669</th>\n",
       "      <td>News</td>\n",
       "      <td>[clinton, campaign, manager, robby, mook, pred...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150741</th>\n",
       "      <td>Trump Support</td>\n",
       "      <td>[love, bring, best, moderate, peace, loving, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150744</th>\n",
       "      <td>Trump Support</td>\n",
       "      <td>[devout, muslim, want, work, cair, orchestrate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150747</th>\n",
       "      <td>Trump Support</td>\n",
       "      <td>[moderate, muslim, egypt, christian, accuse, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150771</th>\n",
       "      <td>Sports</td>\n",
       "      <td>[pin, tail, toupee]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150776</th>\n",
       "      <td>Trump Support</td>\n",
       "      <td>[moderate, muslim, egypt, christian, accuse, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150786</th>\n",
       "      <td>Trump Support</td>\n",
       "      <td>[president, trump, address, joint, session]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150795</th>\n",
       "      <td>Sports</td>\n",
       "      <td>[fangirling, try, scream, time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150815</th>\n",
       "      <td>Trump Support</td>\n",
       "      <td>[devout, muslim, want, work, believe, cair, or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150827</th>\n",
       "      <td>Sports</td>\n",
       "      <td>[fangirling, try, scream, time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150834</th>\n",
       "      <td>Patriot</td>\n",
       "      <td>[allow, carry, gun, vagina]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150849</th>\n",
       "      <td>Entertainment</td>\n",
       "      <td>[what, emmy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150857</th>\n",
       "      <td>Patriot</td>\n",
       "      <td>[samuel, colt, born, creator, barreled, colt, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150867</th>\n",
       "      <td>Entertainment</td>\n",
       "      <td>[sayin, try, internet]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>738502 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              hashtags                                         tweet_text\n",
       "33                News             [john, carroll, university, get, gift]\n",
       "34                News      [spring, cook, book, healthy, chocolate, bar]\n",
       "37                News  [rocky, river, prepare, more, flooding, city, ...\n",
       "39                News  [forecast, cooler, mid, week, cooler, isolated...\n",
       "40                News       [brother, farook, decorate, veteran, killer]\n",
       "42                News  [arrest, connection, art, festival, theft, arr...\n",
       "44                News  [director, craven, die, scream, filmmaker, die...\n",
       "45              Sports  [secretariat, dominate, american, pharoah, bel...\n",
       "46                News         [black, box, miss, german, airline, crash]\n",
       "48              Sports  [dale, earnhardt, get, engage, girlfriend, amy...\n",
       "50              Sports  [brown, mingo, undergo, surgery, brown, lineba...\n",
       "51                News                       [avenger, explode, thursday]\n",
       "52               Crime  [clean, sweep, vacuum, cleaner, thief, mayfiel...\n",
       "53                News  [state, senate, expect, vote, common, core, bill]\n",
       "56              Sports  [johns, take, trust, new, report, look, nfls, ...\n",
       "57               Crime  [cleveland, height, police, look, man, shot, h...\n",
       "58               Crime  [medina, man, use, trash, hide, smell, daughte...\n",
       "60       Entertainment                             [best, cast, director]\n",
       "67       Entertainment                            [take, hand, add, plan]\n",
       "71          Anti-Trump          [voting, voting, gop, nominee, vote, htt]\n",
       "86           Fukushima                         [god, help, sound, absurd]\n",
       "88           Fukushima  [thermal, nuclear, power, plant, ukraine, inte...\n",
       "89                News  [richard, kivett, elected, mayor, village, sun...\n",
       "102               News     [yeman, fighter, take, aden, tawahi, district]\n",
       "103               News     [yeman, fighter, take, aden, tawahi, district]\n",
       "104               News  [agriculture, equestrian, center, take, shape,...\n",
       "105               News  [agriculture, equestrian, center, take, shape,...\n",
       "106               News  [arrest, polouse, drive, stolen, vehicle, dwi,...\n",
       "107               News  [saint, patriot, joint, practice, bring, chang...\n",
       "110               News                  [clinton, campaign, arrest, make]\n",
       "...                ...                                                ...\n",
       "2150486  Black Support      [common, color, blue, youre, blackthen, know]\n",
       "2150488  Trump Support      [worst, america, guess, hate, half, american]\n",
       "2150489     Anti-Trump      [worst, america, guess, hate, half, american]\n",
       "2150518         Sports                    [fangirling, try, scream, time]\n",
       "2150520  Trump Support          [president, put, safety, american, first]\n",
       "2150558        Patriot            [hold, hand, battle, cuddle, airstrike]\n",
       "2150564        Patriot                     [divorce, man, friendly, fire]\n",
       "2150570         Sports                    [fangirling, try, scream, time]\n",
       "2150586  Trump Support  [truth, longer, allow, british, cop, fire, cri...\n",
       "2150603  Black Support             [victory, fight, isnt, good, momentum]\n",
       "2150606  Trump Support  [remember, take, salary, accept, third, paycheck]\n",
       "2150614           News    [refuse, reveal, source, wont, seem, confident]\n",
       "2150626        Patriot                        [allow, carry, gun, vagina]\n",
       "2150638        Patriot                             [male, actually, work]\n",
       "2150651        Patriot                    [get, bullet, bullet, man, get]\n",
       "2150667     Anti-Trump                                     [come, really]\n",
       "2150669           News  [clinton, campaign, manager, robby, mook, pred...\n",
       "2150741  Trump Support  [love, bring, best, moderate, peace, loving, c...\n",
       "2150744  Trump Support  [devout, muslim, want, work, cair, orchestrate...\n",
       "2150747  Trump Support  [moderate, muslim, egypt, christian, accuse, d...\n",
       "2150771         Sports                                [pin, tail, toupee]\n",
       "2150776  Trump Support  [moderate, muslim, egypt, christian, accuse, d...\n",
       "2150786  Trump Support        [president, trump, address, joint, session]\n",
       "2150795         Sports                    [fangirling, try, scream, time]\n",
       "2150815  Trump Support  [devout, muslim, want, work, believe, cair, or...\n",
       "2150827         Sports                    [fangirling, try, scream, time]\n",
       "2150834        Patriot                        [allow, carry, gun, vagina]\n",
       "2150849  Entertainment                                       [what, emmy]\n",
       "2150857        Patriot  [samuel, colt, born, creator, barreled, colt, ...\n",
       "2150867  Entertainment                             [sayin, try, internet]\n",
       "\n",
       "[738502 rows x 2 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(df,i,batch_size):\n",
    "    batches = []\n",
    "    results = []\n",
    "    texts = df.tweet_text[i*batch_size:i*batch_size+batch_size]\n",
    "    categories = df.hashtags[i*batch_size:i*batch_size+batch_size]\n",
    "    for text in texts:\n",
    "        layer = np.zeros(total_words,dtype=float) # total words is not defined\n",
    "        for word in text:\n",
    "            layer[word2index[word[2:-1].lower()]] += 1\n",
    "\n",
    "        batches.append(layer)\n",
    "\n",
    "    for category in categories:\n",
    "        index_y = -1\n",
    "        if category == 'General News':\n",
    "            index_y = 0\n",
    "        elif category == 'Sports':\n",
    "            index_y = 1\n",
    "        elif category == 'Crime':\n",
    "            index_y = 2\n",
    "        elif category == 'Technology':\n",
    "            index_y = 3\n",
    "        elif category == 'Entertainment':\n",
    "            index_y = 4\n",
    "        elif category == 'Anti-Hillary':\n",
    "            index_y = 5\n",
    "        elif category == 'Fukushima':\n",
    "            index_y = 6\n",
    "        elif category == 'Foreign Countries':\n",
    "            index_y = 7\n",
    "        elif category == 'Trump Support':\n",
    "            index_y = 8\n",
    "        elif category == 'Gun Related':\n",
    "            index_y = 9\n",
    "        elif category == 'Video Games':\n",
    "            index_y = 10\n",
    "        elif category == 'Health':\n",
    "            index_y = 11\n",
    "        elif category == 'Business':\n",
    "            index_y = 12\n",
    "        elif category == 'Black Support':\n",
    "            index_y = 13\n",
    "        elif category == 'Anti-Islam':\n",
    "            index_y = 14\n",
    "        results.append(index_y)\n",
    "\n",
    "\n",
    "    return np.array(batches),np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "num_epochs = 4\n",
    "batch_size = 150\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "hidden_size = 100      # 1st layer and 2nd layer number of features\n",
    "input_size = total_words # Words in vocab\n",
    "num_classes = 15        # Categories: graphics, sci.space and baseball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "inputs = Variable(torch.randn(2, 5), requires_grad=True)\n",
    "target = Variable(torch.LongTensor(2).random_(5))\n",
    "output = loss(inputs, target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do Neural Network with Linear Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(input_size,hidden_size, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer_2 = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer_1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer_2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.output_layer(out)\n",
    "        return out\n",
    "    \n",
    "net = OurNet(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "# scheduler = ReduceLROnPlateau(optimizer, 'min') # for validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the Model\n",
    "for epoch in range(num_epochs):\n",
    "    total_batch = int(len(filtered.tweet_text)/batch_size)\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "        batch_x,batch_y = get_batch(filtered,i,batch_size)\n",
    "        articles = Variable(torch.FloatTensor(batch_x))\n",
    "        labels = Variable(torch.LongTensor(batch_y))\n",
    "        #print(\"articles\",articles)\n",
    "        #print(batch_x, labels)\n",
    "        #print(\"size labels\",labels.size())\n",
    "\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()  # zero the gradient buffer\n",
    "        outputs = net(articles)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                   %(epoch+1, num_epochs, i+1, len(filtered.tweet_text)//batch_size, loss.data[0]))\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize(['I need to get healthy who wants to go to the gym', 'Yesterday I saw a great football game'], min_length=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = filtered.loc[2150651].copy()\n",
    "\n",
    "test_set\n",
    "\n",
    "test_set.head()\n",
    "batch_x_test,batch_y_test = get_batch(test_set,0,2)\n",
    "articles = Variable(torch.FloatTensor(batch_x_test))\n",
    "labels = torch.LongTensor(batch_y_test)\n",
    "outputs = net(articles)\n",
    "print(outputs)\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "print(predicted)\n",
    "\n",
    "# torch.save(net.state_dict(), 'supervised_linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Convolutional Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(input_size, input_size, kernel_size=2) # dimensions need to be fixed\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=2)\n",
    "        self.mp = nn.MaxPool1d(2)\n",
    "        self.fc = nn.Linear(100, 15)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.mp(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.relu(self.mp(self.conv2(x)))\n",
    "        x = x.view(in_size, -1)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x)\n",
    "        \n",
    "convnet = ConvNet(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the Model\n",
    "for epoch in range(num_epochs):\n",
    "    total_batch = int(len(filtered.tweet_text)/batch_size)\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "        batch_x,batch_y = get_batch(filtered,i,batch_size)\n",
    "        articles = Variable(torch.FloatTensor(batch_x))\n",
    "        labels = Variable(torch.LongTensor(batch_y))\n",
    "        #print(\"articles\",articles)\n",
    "        #print(batch_x, labels)\n",
    "        #print(\"size labels\",labels.size())\n",
    "\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()  # zero the gradient buffer\n",
    "        outputs = convnet(articles)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                   %(epoch+1, num_epochs, i+1, len(filtered.tweet_text)//batch_size, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
