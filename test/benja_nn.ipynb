{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import gensim\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from LDA_helpers import process_tweets\n",
    "from helpers import *\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Conv1D, MaxPooling1D, GlobalAveragePooling1D, Flatten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IRAhandle_tweets_4.csv', 'IRAhandle_tweets_5.csv', 'rus_troll_tweet_stats.csv', 'IRAhandle_tweets_7.csv', 'IRAhandle_tweets_6.csv', 'rus_troll_user.csv', 'IRAhandle_tweets_2.csv', '.DS_Store', 'IRAhandle_tweets_3.csv', 'IRAhandle_tweets_1.csv', 'line_to_author.pickle', 'iran_troll_tweet_stats.csv', 'rus_troll_tweet_text.csv', '.gitignore', 'iran_troll_tweet_text.csv', 'tweet_author_df.pickle', 'iran_troll_user.csv', 'hashtags.txt', 'rus_troll_tweet_metadata.csv', 'iran_troll_tweet_metadata.csv', 'IRAhandle_tweets_8.csv', 'IRAhandle_tweets_9.csv']\n",
      "['Unknown_tweets.txt', 'test_indices.npy', '.DS_Store', 'cleaned_RightTroll_tweets.txt', 'cleaned_Unknown_tweets.txt', 'cleaned_HashtagGamer_tweets.txt', 'benja.pkl', 'LeftTroll_tweets.txt', 'NonEnglish_tweets.txt', 'NewsFeed_tweets.txt', 'cleaned_Fearmonger_tweets.txt', 'benja.csv', 'cleaned_NewsFeed_tweets.txt', '.gitignore', 'training_indices.npy', 'cleaned_NonEnglish_tweets.txt', 'Fearmonger_tweets.txt', 'HashtagGamer_tweets.txt', 'weatherstuff.csv', 'cleaned_LeftTroll_tweets.txt', 'tweets_n_hashtags.csv', 'RightTroll_tweets.txt']\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = access_folder()\n",
    "GENERATED_DATA_PATH = access_folder('generated')\n",
    "print(os.listdir(DATA_PATH))\n",
    "print(os.listdir(GENERATED_DATA_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>Topic Ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>News</td>\n",
       "      <td>['john', 'carroll', 'university', 'get', 'gift']</td>\n",
       "      <td>news</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>News</td>\n",
       "      <td>['spring', 'cook', 'book', 'healthy', 'chocola...</td>\n",
       "      <td>local</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>News</td>\n",
       "      <td>['rocky', 'river', 'prepare', 'more', 'floodin...</td>\n",
       "      <td>news</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>News</td>\n",
       "      <td>['forecast', 'cooler', 'mid', 'week', 'cooler'...</td>\n",
       "      <td>news</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>News</td>\n",
       "      <td>['brother', 'farook', 'decorate', 'veteran', '...</td>\n",
       "      <td>TopNews</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>News</td>\n",
       "      <td>['arrest', 'connection', 'art', 'festival', 't...</td>\n",
       "      <td>news</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>News</td>\n",
       "      <td>['director', 'craven', 'die', 'scream', 'filmm...</td>\n",
       "      <td>news</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Sports</td>\n",
       "      <td>['secretariat', 'dominate', 'american', 'pharo...</td>\n",
       "      <td>sports</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>News</td>\n",
       "      <td>['black', 'box', 'miss', 'german', 'airline', ...</td>\n",
       "      <td>local</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Sports</td>\n",
       "      <td>['dale', 'earnhardt', 'get', 'engage', 'girlfr...</td>\n",
       "      <td>sports</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Sports</td>\n",
       "      <td>['brown', 'mingo', 'undergo', 'surgery', 'brow...</td>\n",
       "      <td>sports</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>News</td>\n",
       "      <td>['avenger', 'explode', 'thursday']</td>\n",
       "      <td>local</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Crime</td>\n",
       "      <td>['clean', 'sweep', 'vacuum', 'cleaner', 'thief...</td>\n",
       "      <td>crime</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>News</td>\n",
       "      <td>['state', 'senate', 'expect', 'vote', 'common'...</td>\n",
       "      <td>local</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Sports</td>\n",
       "      <td>['johns', 'take', 'trust', 'new', 'report', 'l...</td>\n",
       "      <td>sports</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Crime</td>\n",
       "      <td>['cleveland', 'height', 'police', 'look', 'man...</td>\n",
       "      <td>crime</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Crime</td>\n",
       "      <td>['medina', 'man', 'use', 'trash', 'hide', 'sme...</td>\n",
       "      <td>crime</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Entertainment</td>\n",
       "      <td>['best', 'cast', 'director']</td>\n",
       "      <td>MyEmmyNominationWouldBe</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Entertainment</td>\n",
       "      <td>['take', 'hand', 'add', 'plan']</td>\n",
       "      <td>rap</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Anti-Trump</td>\n",
       "      <td>['voting', 'voting', 'gop', 'nominee', 'vote',...</td>\n",
       "      <td>NeverHillary</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Fukushima</td>\n",
       "      <td>['god', 'help', 'sound', 'absurd']</td>\n",
       "      <td>FukushimaAgain</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Fukushima</td>\n",
       "      <td>['thermal', 'nuclear', 'power', 'plant', 'ukra...</td>\n",
       "      <td>FukushimaAgain</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>News</td>\n",
       "      <td>['richard', 'kivett', 'elected', 'mayor', 'vil...</td>\n",
       "      <td>news</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>News</td>\n",
       "      <td>['yeman', 'fighter', 'take', 'aden', 'tawahi',...</td>\n",
       "      <td>world</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>News</td>\n",
       "      <td>['yeman', 'fighter', 'take', 'aden', 'tawahi',...</td>\n",
       "      <td>news</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>News</td>\n",
       "      <td>['agriculture', 'equestrian', 'center', 'take'...</td>\n",
       "      <td>local</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>News</td>\n",
       "      <td>['agriculture', 'equestrian', 'center', 'take'...</td>\n",
       "      <td>news</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>News</td>\n",
       "      <td>['arrest', 'polouse', 'drive', 'stolen', 'vehi...</td>\n",
       "      <td>news</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>News</td>\n",
       "      <td>['saint', 'patriot', 'joint', 'practice', 'bri...</td>\n",
       "      <td>news</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>News</td>\n",
       "      <td>['clinton', 'campaign', 'arrest', 'make']</td>\n",
       "      <td>breaking</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150486</th>\n",
       "      <td>Black Support</td>\n",
       "      <td>['common', 'color', 'blue', 'youre', 'blackthe...</td>\n",
       "      <td>blacklivesmatter</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150488</th>\n",
       "      <td>Trump Support</td>\n",
       "      <td>['worst', 'america', 'guess', 'hate', 'half', ...</td>\n",
       "      <td>TrumpTrain</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150489</th>\n",
       "      <td>Anti-Trump</td>\n",
       "      <td>['worst', 'america', 'guess', 'hate', 'half', ...</td>\n",
       "      <td>Hillary</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150518</th>\n",
       "      <td>Sports</td>\n",
       "      <td>['fangirling', 'try', 'scream', 'time']</td>\n",
       "      <td>MyOlympicSportWouldBe</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150520</th>\n",
       "      <td>Trump Support</td>\n",
       "      <td>['president', 'put', 'safety', 'american', 'fi...</td>\n",
       "      <td>MAGA</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150558</th>\n",
       "      <td>Patriot</td>\n",
       "      <td>['hold', 'hand', 'battle', 'cuddle', 'airstrike']</td>\n",
       "      <td>ArmyWomenProblems</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150564</th>\n",
       "      <td>Patriot</td>\n",
       "      <td>['divorce', 'man', 'friendly', 'fire']</td>\n",
       "      <td>ArmyWomenProblems</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150570</th>\n",
       "      <td>Sports</td>\n",
       "      <td>['fangirling', 'try', 'scream', 'time']</td>\n",
       "      <td>MyOlympicSportWouldBe</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150586</th>\n",
       "      <td>Trump Support</td>\n",
       "      <td>['truth', 'longer', 'allow', 'british', 'cop',...</td>\n",
       "      <td>tcot</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150603</th>\n",
       "      <td>Black Support</td>\n",
       "      <td>['victory', 'fight', 'isnt', 'good', 'momentum']</td>\n",
       "      <td>blacklivesmatter</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150606</th>\n",
       "      <td>Trump Support</td>\n",
       "      <td>['remember', 'take', 'salary', 'accept', 'thir...</td>\n",
       "      <td>MAGA</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150614</th>\n",
       "      <td>News</td>\n",
       "      <td>['refuse', 'reveal', 'source', 'wont', 'seem',...</td>\n",
       "      <td>Breaking</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150626</th>\n",
       "      <td>Patriot</td>\n",
       "      <td>['allow', 'carry', 'gun', 'vagina']</td>\n",
       "      <td>ArmyWomenProblems</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150638</th>\n",
       "      <td>Patriot</td>\n",
       "      <td>['male', 'actually', 'work']</td>\n",
       "      <td>ArmyWomenProblems</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150651</th>\n",
       "      <td>Patriot</td>\n",
       "      <td>['get', 'bullet', 'bullet', 'man', 'get']</td>\n",
       "      <td>ArmyWomenProblems</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150667</th>\n",
       "      <td>Anti-Trump</td>\n",
       "      <td>['come', 'really']</td>\n",
       "      <td>CrookedHillary</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150669</th>\n",
       "      <td>News</td>\n",
       "      <td>['clinton', 'campaign', 'manager', 'robby', 'm...</td>\n",
       "      <td>Breaking</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150741</th>\n",
       "      <td>Trump Support</td>\n",
       "      <td>['love', 'bring', 'best', 'moderate', 'peace',...</td>\n",
       "      <td>tcot</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150744</th>\n",
       "      <td>Trump Support</td>\n",
       "      <td>['devout', 'muslim', 'want', 'work', 'cair', '...</td>\n",
       "      <td>tcot</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150747</th>\n",
       "      <td>Trump Support</td>\n",
       "      <td>['moderate', 'muslim', 'egypt', 'christian', '...</td>\n",
       "      <td>tcot</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150771</th>\n",
       "      <td>Sports</td>\n",
       "      <td>['pin', 'tail', 'toupee']</td>\n",
       "      <td>MyOlympicSportWouldBe</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150776</th>\n",
       "      <td>Trump Support</td>\n",
       "      <td>['moderate', 'muslim', 'egypt', 'christian', '...</td>\n",
       "      <td>tcot</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150786</th>\n",
       "      <td>Trump Support</td>\n",
       "      <td>['president', 'trump', 'address', 'joint', 'se...</td>\n",
       "      <td>MAGA</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150795</th>\n",
       "      <td>Sports</td>\n",
       "      <td>['fangirling', 'try', 'scream', 'time']</td>\n",
       "      <td>MyOlympicSportWouldBe</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150815</th>\n",
       "      <td>Trump Support</td>\n",
       "      <td>['devout', 'muslim', 'want', 'work', 'believe'...</td>\n",
       "      <td>tcot</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150827</th>\n",
       "      <td>Sports</td>\n",
       "      <td>['fangirling', 'try', 'scream', 'time']</td>\n",
       "      <td>MyOlympicSportWouldBe</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150834</th>\n",
       "      <td>Patriot</td>\n",
       "      <td>['allow', 'carry', 'gun', 'vagina']</td>\n",
       "      <td>ArmyWomenProblems</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150849</th>\n",
       "      <td>Entertainment</td>\n",
       "      <td>['what', 'emmy']</td>\n",
       "      <td>MyEmmyNominationWouldBe</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150857</th>\n",
       "      <td>Patriot</td>\n",
       "      <td>['samuel', 'colt', 'born', 'creator', 'barrele...</td>\n",
       "      <td>2A</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150867</th>\n",
       "      <td>Entertainment</td>\n",
       "      <td>['sayin', 'try', 'internet']</td>\n",
       "      <td>NowPlaying</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>738502 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Topic                                         tweet_text  \\\n",
       "33                News   ['john', 'carroll', 'university', 'get', 'gift']   \n",
       "34                News  ['spring', 'cook', 'book', 'healthy', 'chocola...   \n",
       "37                News  ['rocky', 'river', 'prepare', 'more', 'floodin...   \n",
       "39                News  ['forecast', 'cooler', 'mid', 'week', 'cooler'...   \n",
       "40                News  ['brother', 'farook', 'decorate', 'veteran', '...   \n",
       "42                News  ['arrest', 'connection', 'art', 'festival', 't...   \n",
       "44                News  ['director', 'craven', 'die', 'scream', 'filmm...   \n",
       "45              Sports  ['secretariat', 'dominate', 'american', 'pharo...   \n",
       "46                News  ['black', 'box', 'miss', 'german', 'airline', ...   \n",
       "48              Sports  ['dale', 'earnhardt', 'get', 'engage', 'girlfr...   \n",
       "50              Sports  ['brown', 'mingo', 'undergo', 'surgery', 'brow...   \n",
       "51                News                 ['avenger', 'explode', 'thursday']   \n",
       "52               Crime  ['clean', 'sweep', 'vacuum', 'cleaner', 'thief...   \n",
       "53                News  ['state', 'senate', 'expect', 'vote', 'common'...   \n",
       "56              Sports  ['johns', 'take', 'trust', 'new', 'report', 'l...   \n",
       "57               Crime  ['cleveland', 'height', 'police', 'look', 'man...   \n",
       "58               Crime  ['medina', 'man', 'use', 'trash', 'hide', 'sme...   \n",
       "60       Entertainment                       ['best', 'cast', 'director']   \n",
       "67       Entertainment                    ['take', 'hand', 'add', 'plan']   \n",
       "71          Anti-Trump  ['voting', 'voting', 'gop', 'nominee', 'vote',...   \n",
       "86           Fukushima                 ['god', 'help', 'sound', 'absurd']   \n",
       "88           Fukushima  ['thermal', 'nuclear', 'power', 'plant', 'ukra...   \n",
       "89                News  ['richard', 'kivett', 'elected', 'mayor', 'vil...   \n",
       "102               News  ['yeman', 'fighter', 'take', 'aden', 'tawahi',...   \n",
       "103               News  ['yeman', 'fighter', 'take', 'aden', 'tawahi',...   \n",
       "104               News  ['agriculture', 'equestrian', 'center', 'take'...   \n",
       "105               News  ['agriculture', 'equestrian', 'center', 'take'...   \n",
       "106               News  ['arrest', 'polouse', 'drive', 'stolen', 'vehi...   \n",
       "107               News  ['saint', 'patriot', 'joint', 'practice', 'bri...   \n",
       "110               News          ['clinton', 'campaign', 'arrest', 'make']   \n",
       "...                ...                                                ...   \n",
       "2150486  Black Support  ['common', 'color', 'blue', 'youre', 'blackthe...   \n",
       "2150488  Trump Support  ['worst', 'america', 'guess', 'hate', 'half', ...   \n",
       "2150489     Anti-Trump  ['worst', 'america', 'guess', 'hate', 'half', ...   \n",
       "2150518         Sports            ['fangirling', 'try', 'scream', 'time']   \n",
       "2150520  Trump Support  ['president', 'put', 'safety', 'american', 'fi...   \n",
       "2150558        Patriot  ['hold', 'hand', 'battle', 'cuddle', 'airstrike']   \n",
       "2150564        Patriot             ['divorce', 'man', 'friendly', 'fire']   \n",
       "2150570         Sports            ['fangirling', 'try', 'scream', 'time']   \n",
       "2150586  Trump Support  ['truth', 'longer', 'allow', 'british', 'cop',...   \n",
       "2150603  Black Support   ['victory', 'fight', 'isnt', 'good', 'momentum']   \n",
       "2150606  Trump Support  ['remember', 'take', 'salary', 'accept', 'thir...   \n",
       "2150614           News  ['refuse', 'reveal', 'source', 'wont', 'seem',...   \n",
       "2150626        Patriot                ['allow', 'carry', 'gun', 'vagina']   \n",
       "2150638        Patriot                       ['male', 'actually', 'work']   \n",
       "2150651        Patriot          ['get', 'bullet', 'bullet', 'man', 'get']   \n",
       "2150667     Anti-Trump                                 ['come', 'really']   \n",
       "2150669           News  ['clinton', 'campaign', 'manager', 'robby', 'm...   \n",
       "2150741  Trump Support  ['love', 'bring', 'best', 'moderate', 'peace',...   \n",
       "2150744  Trump Support  ['devout', 'muslim', 'want', 'work', 'cair', '...   \n",
       "2150747  Trump Support  ['moderate', 'muslim', 'egypt', 'christian', '...   \n",
       "2150771         Sports                          ['pin', 'tail', 'toupee']   \n",
       "2150776  Trump Support  ['moderate', 'muslim', 'egypt', 'christian', '...   \n",
       "2150786  Trump Support  ['president', 'trump', 'address', 'joint', 'se...   \n",
       "2150795         Sports            ['fangirling', 'try', 'scream', 'time']   \n",
       "2150815  Trump Support  ['devout', 'muslim', 'want', 'work', 'believe'...   \n",
       "2150827         Sports            ['fangirling', 'try', 'scream', 'time']   \n",
       "2150834        Patriot                ['allow', 'carry', 'gun', 'vagina']   \n",
       "2150849  Entertainment                                   ['what', 'emmy']   \n",
       "2150857        Patriot  ['samuel', 'colt', 'born', 'creator', 'barrele...   \n",
       "2150867  Entertainment                       ['sayin', 'try', 'internet']   \n",
       "\n",
       "                         hashtag  Topic Ids  \n",
       "33                          news          8  \n",
       "34                         local          8  \n",
       "37                          news          8  \n",
       "39                          news          8  \n",
       "40                       TopNews          8  \n",
       "42                          news          8  \n",
       "44                          news          8  \n",
       "45                        sports         10  \n",
       "46                         local          8  \n",
       "48                        sports         10  \n",
       "50                        sports         10  \n",
       "51                         local          8  \n",
       "52                         crime          3  \n",
       "53                         local          8  \n",
       "56                        sports         10  \n",
       "57                         crime          3  \n",
       "58                         crime          3  \n",
       "60       MyEmmyNominationWouldBe          4  \n",
       "67                           rap          4  \n",
       "71                  NeverHillary          1  \n",
       "86                FukushimaAgain          6  \n",
       "88                FukushimaAgain          6  \n",
       "89                          news          8  \n",
       "102                        world          8  \n",
       "103                         news          8  \n",
       "104                        local          8  \n",
       "105                         news          8  \n",
       "106                         news          8  \n",
       "107                         news          8  \n",
       "110                     breaking          8  \n",
       "...                          ...        ...  \n",
       "2150486         blacklivesmatter          2  \n",
       "2150488               TrumpTrain         11  \n",
       "2150489                  Hillary          1  \n",
       "2150518    MyOlympicSportWouldBe         10  \n",
       "2150520                     MAGA         11  \n",
       "2150558        ArmyWomenProblems          9  \n",
       "2150564        ArmyWomenProblems          9  \n",
       "2150570    MyOlympicSportWouldBe         10  \n",
       "2150586                     tcot         11  \n",
       "2150603         blacklivesmatter          2  \n",
       "2150606                     MAGA         11  \n",
       "2150614                 Breaking          8  \n",
       "2150626        ArmyWomenProblems          9  \n",
       "2150638        ArmyWomenProblems          9  \n",
       "2150651        ArmyWomenProblems          9  \n",
       "2150667           CrookedHillary          1  \n",
       "2150669                 Breaking          8  \n",
       "2150741                     tcot         11  \n",
       "2150744                     tcot         11  \n",
       "2150747                     tcot         11  \n",
       "2150771    MyOlympicSportWouldBe         10  \n",
       "2150776                     tcot         11  \n",
       "2150786                     MAGA         11  \n",
       "2150795    MyOlympicSportWouldBe         10  \n",
       "2150815                     tcot         11  \n",
       "2150827    MyOlympicSportWouldBe         10  \n",
       "2150834        ArmyWomenProblems          9  \n",
       "2150849  MyEmmyNominationWouldBe          4  \n",
       "2150857                       2A          9  \n",
       "2150867               NowPlaying          4  \n",
       "\n",
       "[738502 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_csv(GENERATED_DATA_PATH + 'tweets_n_hashtags.csv', index_col=0)\n",
    "data_df['Topic'] = data_df['Topic'].astype('category')\n",
    "data_df['Topic Ids'] = data_df['Topic'].cat.codes\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 524589, 1795012, 1642773, ...,  811295, 1303909,  778210])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_indices = np.load(GENERATED_DATA_PATH + 'test_indices.npy')\n",
    "test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.tweet_text = data_df.tweet_text.apply(lambda x: x[1:-1]).replace(\"b'\", '', regex=True).replace(\"'\", '', regex=True)\n",
    "data_df.tweet_text = data_df.tweet_text.apply(lambda x: x.split(', '))\n",
    "categories = ['News', 'Sports', 'Crime', 'Fukushima', 'Entertainment', \n",
    "              'Anti-Trump', 'Patriot', 'Trump Support',\n",
    "              'Foreign Countries', 'Health',\n",
    "              'Black Support', 'Anti-Islam']\n",
    "from collections import Counter\n",
    "\n",
    "vocab = Counter()\n",
    "\n",
    "for text in data_df.tweet_text:\n",
    "    for word in text: ## change this because the texts are already a list of words\n",
    "        vocab[word.lower()]+=1\n",
    "\n",
    "total_words = len(vocab)\n",
    "\n",
    "def get_word_2_index(vocab):\n",
    "    word2index = {}\n",
    "    for i,word in enumerate(vocab):\n",
    "        word2index[word.lower()] = i\n",
    "\n",
    "    return word2index\n",
    "\n",
    "word2index = get_word_2_index(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_2_1_hot_encoding(tweets, dictionary):\n",
    "    x_data = []\n",
    "    for tweet in tweets:\n",
    "        line = np.zeros(len(dictionary.keys()))\n",
    "        for word in tweet:\n",
    "            line[dictionary[word]] = 1\n",
    "        x_data.append(line)\n",
    "    return x_data\n",
    "\n",
    "def neural_net(shape):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(64, 3, activation='relu', input_shape=(shape, 1)))\n",
    "    model.add(Conv1D(64, 3, activation='relu'))\n",
    "    model.add(MaxPooling1D(13))\n",
    "    model.add(Flatten())\n",
    "    #model.add(Dense(512, input_shape=(shape, ), activation='relu' ))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(12 , activation='softmax', ))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_training_indices(data_indexes, test_indexes):\n",
    "    training_indexes = []\n",
    "    for index in data_indexes:\n",
    "        if index not in test_indexes:\n",
    "            training_indexes.append(index)\n",
    "    return np.array(training_indexes)\n",
    "\n",
    "def train(model , training_indices, data, class_weight, dictionary):\n",
    "    splits = np.array_split(training_indices, 50)\n",
    "    split_i = 1\n",
    "    for split in splits:\n",
    "        print('split: {}/{}'.format(split_i, len(splits)))\n",
    "        train_data = data.loc[split,:]\n",
    "        train_data['X'] = data_2_1_hot_encoding(train_data.tweet_text, dictionary)\n",
    "        X_train = np.zeros((len(split, ),len(train_data.X.values[0])))\n",
    "        y_train = []\n",
    "        i = 0\n",
    "        for train_index in split:\n",
    "            X_train[i,:] = np.array(train_data.loc[train_index,('X')])\n",
    "            y_train.append(data.iloc[train_index]['Topic Ids'])\n",
    "            i = i + 1\n",
    "        print(X_train[0,:].sum())\n",
    "        one_hot_labels = keras.utils.to_categorical(np.array(y_train), num_classes=12)\n",
    "        model.fit(np.reshape(X_train, (np.shape(X_train)[0], np.shape(X_train)[1], 1)), one_hot_labels, epochs=10, batch_size=32, verbose=2, validation_split=0.2,\n",
    "                  class_weight = class_weight)\n",
    "        split_i = split_i + 1\n",
    "    model.save(GENERATED_DATA_PATH + 'model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_df['X'] = data_2_1_hot_encoding(data_df.tweet_text, word2index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     33,      34,      37, ..., 2150827, 2150834, 2150857])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_indices = get_training_indices(data_df.index.values, test_indices)\n",
    "training_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(GENERATED_DATA_PATH + 'training_indices.npy',training_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_32 (Conv1D)           (None, 66216, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 66214, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 5093, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 325952)            0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 325952)            0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 12)                3911436   \n",
      "=================================================================\n",
      "Total params: 3,924,044\n",
      "Trainable params: 3,924,044\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "split: 1/50\n",
      "5.0\n",
      "Train on 9453 samples, validate on 2364 samples\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "model = neural_net(len(word2index))\n",
    "s = data_df['Topic Ids'].value_counts()/data_df['Topic Ids'].sum()\n",
    "train(model , training_indices, data_df, s.to_dict(), word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
