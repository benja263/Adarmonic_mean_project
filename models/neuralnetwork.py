# -*- coding: utf-8 -*-
"""NN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T7AzUMOcH_vKE5hNcqYdckKQ4wkOAO-3
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import re
# from helpers import *
# from tensorflow.contrib import learn
import torch
from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
from torch.optim.lr_scheduler import ReduceLROnPlateau


from google.colab import files
def getLocalFiles():
    _files = files.upload()
    if len(_files) >0:
       for k,v in _files.items():
          open(k,'wb').write(v)
getLocalFiles()

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
print(device)

filtered = pd.read_csv('tweets_n_hashtags.csv', index_col = 0)


filtered.tweet_text = filtered.tweet_text.apply(lambda x: x[1:-1]).replace("b'", '', regex=True).replace("'", '', regex=True)
filtered.tweet_text = filtered.tweet_text.apply(lambda x: x.split(', '))

filtered = filtered[filtered.hashtags != 'News']

filtered.head()

# Set inputs to vectors
categories = ['Sports', 'Crime', 'Patriot', 'Entertainment', 
              'Anti-Trump', 'Fukushima', 'Foreign Countries',
              'Trump Support', 'Anti-Islam',
              'Black Support', 'Health']
from collections import Counter

vocab = Counter()

for text in filtered.tweet_text:
    for word in text: ## change this because the texts are already a list of words
        vocab[word.lower()]+=1

total_words = len(vocab)

def get_word_2_index(vocab):
    word2index = {}
    for i,word in enumerate(vocab):
        word2index[word.lower()] = i

    return word2index

word2index = get_word_2_index(vocab)



total_words

def get_batch(df,i,batch_size):
    batches = []
    results = []
    texts = df.tweet_text[i*batch_size:i*batch_size+batch_size]
    categories = df.hashtags[i*batch_size:i*batch_size+batch_size]
    for text in texts:
        layer = np.zeros(total_words,dtype=float) # total words is not defined
        for word in text:
            layer[word2index[word.lower()]] += 1

        batches.append(layer)

    for category in categories:
        index_y = -1
        if category == 'Sports':
            index_y = 0
        elif category == 'Crime':
            index_y = 1
        elif category == 'Patriot':
            index_y = 2
        elif category == 'Entertainment':
            index_y = 3
        elif category == 'Anti-Trump':
            index_y = 4
        elif category == 'Fukushima':
            index_y = 5
        elif category == 'Foreign Countries':
            index_y = 6
        elif category == 'Trump Support':
            index_y = 7
        elif category == 'Anti-Islam':
            index_y = 8
        elif category == 'Black Support':
            index_y = 9
        elif category == 'Health':
            index_y = 10
        results.append(index_y)


    return np.array(batches),np.array(results)

weights_class = (filtered.hashtags.value_counts().values/filtered.hashtags.value_counts().values.sum())**(-1)

weights_class

# Parameters
learning_rate = 0.001
num_epochs = 5
batch_size = 150
display_step = 1

# Network Parameters
hidden_size1 = 6000
hidden_size2 = 1000
hidden_size3 = 100 # 1st layer and 2nd layer number of features
input_size = total_words # Words in vocab
num_classes = 11       # Categories: graphics, sci.space and baseball

class OurNet(nn.Module):
    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, num_classes):
        super().__init__()
        self.layer_1 = nn.Linear(input_size,hidden_size1, bias=True)
        self.relu = nn.ReLU()
        self.layer_2 = nn.Linear(hidden_size1, hidden_size2, bias=True)
        self.layer_3 = nn.Linear(hidden_size2, hidden_size3, bias=True)
        self.output_layer = nn.Linear(hidden_size3, num_classes, bias=True)

    def forward(self, x):
        out = self.layer_1(x)
        out = self.relu(out)
        out = self.layer_2(out)
        out = self.relu(out)
        out = self.layer_3(out)
        out = self.relu(out)
        out = self.output_layer(out)
        return out
    
net = OurNet(input_size, hidden_size1, hidden_size2, hidden_size3, num_classes)
net.to(device)
# weights = [6.77, 67.62, 38.14, 13.81, 49.69, 30.91, 78.50, 14.15, 72.73, 28.07, 33.59]
class_weights = torch.cuda.FloatTensor(weights_class)
criterion = nn.CrossEntropyLoss(weight=class_weights)
optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)
# scheduler = ReduceLROnPlateau(optimizer, 'min') # for validation set

## Train the Model
for epoch in range(num_epochs):
    total_batch = int(len(filtered.tweet_text)/batch_size)
    # Loop over all batches
    for i in range(total_batch):
        batch_x,batch_y = get_batch(filtered,i,batch_size)
        articles = Variable(torch.cuda.FloatTensor(batch_x))
        labels = Variable(torch.cuda.LongTensor(batch_y))
        #print("articles",articles)
        #print(batch_x, labels)
        #print("size labels",labels.size())

        # Forward + Backward + Optimize
        optimizer.zero_grad()  # zero the gradient buffer
        outputs = net(articles)
        
        
        
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        if (i+1) % 100 == 0:
            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'
#                    %(epoch+1, num_epochs, i+1, len(filtered.tweet_text)//batch_size, loss.data[0]))

# test_set = filtered.loc[2150651].copy()
test_set = ['give', 'mother', 'listen', 'yes']
test_layer = np.zeros(total_words,dtype=float)
for word in test_set:
  test_layer[word2index[word.lower()]] += 1
# test_layer

# test_set.head()
# batch_x_test,batch_y_test = get_batch(test_set,0,2)
articles = Variable(torch.cuda.FloatTensor(test_layer))
# labels = torch.LongTensor(batch_y_test)
outputs = net(articles)
values, idx = torch.max(outputs, 0)
print(categories[idx])
# _, predicted = torch.max(outputs.data, 1)
# print(predicted)

torch.save(net.state_dict(), 'pleasework')

type(net.state_dict())

from google.colab import files

files.download('pleasework')

from google.colab import auth
from googleapiclient.http import MediaFileUpload
from googleapiclient.discovery import build

auth.authenticate_user()